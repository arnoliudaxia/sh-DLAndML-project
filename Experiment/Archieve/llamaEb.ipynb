{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      " You are a helpful assistant.\n",
      "user\n",
      "日本这个国家怎么样\n",
      "assistant\n",
      "日本是一个位于东亚的岛国，拥有悠久的历史和文化。以下是一些关于日本的基本情况：\n",
      "\n",
      "1. 文化：日本文化深受中国、韩国以及西方的影响。它有许多独特的传统艺术形式，如茶道、花道、歌舞伎等。此外，动漫、流行音乐和电子游戏也是日本文化的重要组成部分。\n",
      "\n",
      "2. 经济：日本是世界第三大经济体（按购买力平价计算），以高科技产业和制造业著称。日本企业在汽车制造、电子设备等领域具有很强的竞争力。\n",
      "\n",
      "3. 科技：日本在科技创新方面处于领先地位，在机器人技术、电动汽车、可再生能源等方面取得了显著成就。\n",
      "\n",
      "4. 教育：日本重视教育，国民受教育程度较高。日本大学在全球排名中占有一定位置。\n",
      "\n",
      "5. 自然风光：日本拥有美丽的自然景观，包括富士山、温泉、樱花盛开的季节等。\n",
      "\n",
      "6. 交通：日本拥有发达的公共交通系统，包括高速铁路、地铁和公交等。\n",
      "\n",
      "7. 社会问题：尽管日本经济繁荣，但面临一些社会问题，如人口老龄化、少子化等。\n",
      "\n",
      "总之，日本是一个充满魅力的国家，具有丰富的文化和历史遗产，同时也面临着一些挑战。\n"
     ]
    }
   ],
   "source": [
    "prompt = \"日本这个国家怎么样\"\n",
    "\n",
    "messages = [\n",
    "            # {\"role\": \"system\", \"content\": ' You are a helpful assistant. The user has a language impairment, and their expressions may contain a lot of noise. You need to rephrase their meaning clearly.'},\n",
    "            {\"role\": \"system\", \"content\": ' You are a helpful assistant.'},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "# text=prompt\n",
    "\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "\n",
    "generated_ids = model.generate(model_inputs.input_ids, max_new_tokens=512, do_sample=True)\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      " You are a helpful assistant.\n",
      "user\n",
      "日本这个国家怎么样\n",
      "assistant\n",
      "日本是一个位于东亚的岛国，拥有悠久的历史和文化。以下是一些关于日本的基本情况：\n",
      "\n",
      "1. 文化：日本文化深受中国、韩国以及西方的影响。它有许多独特的传统艺术形式，如茶道、花道、歌舞伎等。此外，动漫、流行音乐和电子游戏也是日本文化的重要组成部分。\n",
      "\n",
      "2. 经济：日本是世界第三大经济体（按购买力平价计算），以高科技产业和制造业著称。日本企业在汽车制造、电子设备等领域具有很强的竞争力。\n",
      "\n",
      "3. 科技：日本在科技创新方面处于领先地位，在机器人技术、电动汽车、可再生能源等方面取得了显著成就。\n",
      "\n",
      "4. 教育：日本重视教育，国民受教育程度较高。日本大学在全球排名中占有一定位置。\n",
      "\n",
      "5. 自然风光：日本拥有美丽的自然景观，包括富士山、温泉、樱花盛开的季节等。\n",
      "\n",
      "6. 交通：日本拥有发达的公共交通系统，包括高速铁路、地铁和公交等。\n",
      "\n",
      "7. 社会问题：尽管日本经济繁荣，但面临一些社会问题，如人口老龄化、少子化等。\n",
      "\n",
      "总之，日本是一个充满魅力的国家，具有丰富的文化和历史遗产，同时也面临着一些挑战。\n"
     ]
    }
   ],
   "source": [
    "prompt = \"日本这个国家怎么样\"\n",
    "\n",
    "messages = [\n",
    "            # {\"role\": \"system\", \"content\": ' You are a helpful assistant. The user has a language impairment, and their expressions may contain a lot of noise. You need to rephrase their meaning clearly.'},\n",
    "            {\"role\": \"system\", \"content\": ' You are a helpful assistant.'},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "# text=prompt\n",
    "\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "predicted_ids=model.generate(inputs_embeds=model.get_input_embeddings()(model_inputs.input_ids) ,\n",
    "                             max_new_tokens=512, do_sample=True)\n",
    "response = tokenizer.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arno/conda/envs/MNE-EEG/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.80it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name ='/home/arno/Projects/EEGDecodingTest/My/LLM/Qwen2.5-7B-Instruct'\n",
    "device=\"cuda\"\n",
    "    \n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def getEmbedding(text, raw=False):\n",
    "        if not raw:\n",
    "                messages = [\n",
    "                        # {\"role\": \"system\", \"content\": ' You are a helpful assistant. The user has a language impairment, and their expressions may contain a lot of noise. You need to rephrase their meaning clearly.'},\n",
    "                        {\"role\": \"system\", \"content\": ' You are a helpful assistant.'},\n",
    "                        {\"role\": \"user\", \"content\": text}\n",
    "                        ]\n",
    "                \n",
    "                text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "        model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "        return model.get_input_embeddings()(model_inputs.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arno/conda/envs/MNE-EEG/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.90it/s]\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今天的天气感觉挺暖和的。外面阳光明媚，虽然有风，但不算冷。今天出门应该没问题，挺舒服的。\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "prompt = '“天…天，天气...嗯，外面，嗯，亮，阳光，太阳，感觉…暖。嗯，风，风有点，嗯，吹，嗯，冷，是，不是...不太...不太冷。啊，今天，今天是，嗯…可以，出门，出去，走，走，没问题。嗯，暖，舒服。”'\n",
    "\n",
    "# messages = [\n",
    "#             {\"role\": \"system\", \"content\": ' You are a helpful assistant. The user has a language impairment, and their expressions may contain a lot of noise. You need to rephrase their meaning clearly.'},\n",
    "#             {\"role\": \"user\", \"content\": prompt}\n",
    "#             ]\n",
    "# text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "# model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "# generated_ids = model.generate(model_inputs.input_ids, max_new_tokens=512, do_sample=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "new_input_embed = torch.cat([getEmbedding('<|im_start|>system\\nYou are a helpful assistant. The user has a language impairment, and their expressions may contain a lot of noise. You need to rephrase their meaning clearly.<|im_end|>\\n<|im_start|>user\\n', raw=True)\n",
    "                             , getEmbedding(prompt, raw=True), \n",
    "                             getEmbedding('<|im_end|>\\n<|im_start|>assistant\\n', raw=True)], dim=1)\n",
    "\n",
    "predicted_ids=model.generate(inputs_embeds=new_input_embed ,\n",
    "                             max_new_tokens=512, do_sample=True)\n",
    "\n",
    "\n",
    "\n",
    "response = tokenizer.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenids' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m characterEmbeddings\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mget_input_embeddings()(torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[43mtokenids\u001b[49m)\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice,\u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenids' is not defined"
     ]
    }
   ],
   "source": [
    "characterEmbeddings=model.get_input_embeddings()(torch.tensor(tokenids).to(device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0004, -0.0034,  0.0013,  ..., -0.0013,  0.0033,  0.0123],\n",
       "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "character_embeddings_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of noisy data: 6.160988414194435e-05\n",
      "Std of noisy data: 0.022888336330652237\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 假设 characterEmbeddings 是一个 PyTorch tensor，形状是 [3479, 3584]\n",
    "# 先展平数据以计算95%的置信区间\n",
    "character_embeddings_flat = characterEmbeddings.view(-1).to(torch.float32) \n",
    "\n",
    "# 计算95%置信区间\n",
    "lower_bound = torch.quantile(character_embeddings_flat, 0.025)\n",
    "upper_bound = torch.quantile(character_embeddings_flat, 0.975)\n",
    "\n",
    "# 计算新的标准差 std_new，基于95%的置信区间\n",
    "std_new = ((upper_bound - lower_bound) / (2 * 1.96)).cpu().item()\n",
    "\n",
    "# 获取均值\n",
    "mean_old = torch.mean(character_embeddings_flat).cpu().item()\n",
    "\n",
    "\n",
    "# 生成与原数据形状相同的噪声\n",
    "noise = torch.normal(mean=mean_old, std=std_new, size=characterEmbeddings.shape)\n",
    "\n",
    "# 生成带噪声的数据\n",
    "noisy_data = characterEmbeddings + noise.to(device=device)\n",
    "\n",
    "# 打印一下结果的均值和标准差，确认噪声分布\n",
    "print(f\"Mean of noisy data: {torch.mean(noisy_data).item()}\")\n",
    "print(f\"Std of noisy data: {torch.std(noisy_data).item()}\")\n",
    "\n",
    "\n",
    "new_input_embed = torch.cat([getEmbedding('<|im_start|>system\\nYou are a helpful assistant. The user has a language impairment, and their expressions may contain a lot of noise. You need to rephrase their meaning clearly.<|im_end|>\\n<|im_start|>user\\n', raw=True)\n",
    "                             , noisy_data.to(torch.bfloat16).unsqueeze(0), \n",
    "                             getEmbedding('<|im_end|>\\n<|im_start|>assistant\\n', raw=True)], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing characters:   0%|          | 0/185828 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1593565/2419851863.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  if torch.tensor(gt_embed).shape[0] == 3584:\n",
      "Processing characters: 100%|██████████| 185828/185828 [2:33:05<00:00, 20.23it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing completed. Responses saved to responses.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total lines: 166984\n",
      "Matching count: 165787\n",
      "Non-matching count: 1197\n",
      "Matching percentage: 99.28%\n",
      "Non-matching percentage: 0.72%\n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MNE-EEG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
