{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:   0%|          | 0/58 [00:00<?, ?it/s]Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x000002459E8549D0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\software\\coding\\anaconda3\\envs\\DLA_ML_project\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n",
      "Processing files:   3%|▎         | 2/58 [00:05<02:22,  2.54s/it]Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x000002459E8549D0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\software\\coding\\anaconda3\\envs\\DLA_ML_project\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n",
      "Processing files: 100%|██████████| 58/58 [02:23<00:00,  2.48s/it]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 数据路径\n",
    "root_dir = 'D:\\\\PythonProject\\\\DLA_ML_project\\\\sh-DLAndML-project\\\\Data\\\\qwen-characterSplit'\n",
    "\n",
    "# 存储 latent 和 tokenid\n",
    "latents = []\n",
    "tokenids = []\n",
    "\n",
    "# 遍历文件夹收集数据\n",
    "total_files = sum(\n",
    "    len([file_name for file_name in os.listdir(os.path.join(root_dir, sub_dir))\n",
    "         if file_name.endswith('.pkl') and 'latent' in file_name and '_tokenid' in file_name and '_tokenEmbedding' in file_name])\n",
    "    for sub_dir in os.listdir(root_dir)\n",
    "    if os.path.isdir(os.path.join(root_dir, sub_dir))\n",
    ")\n",
    "\n",
    "with tqdm(total=total_files, desc=\"Processing files\") as pbar:\n",
    "    for sub_dir in os.listdir(root_dir):\n",
    "        sub_dir_path = os.path.join(root_dir, sub_dir)\n",
    "        if os.path.isdir(sub_dir_path):\n",
    "            for file_name in os.listdir(sub_dir_path):\n",
    "                if file_name.endswith('.pkl') and 'latent' in file_name and '_tokenid' in file_name and '_tokenEmbedding' in file_name:\n",
    "                    file_path = os.path.join(sub_dir_path, file_name)\n",
    "                    with open(file_path, 'rb') as file:\n",
    "                        loaded_data = pickle.load(file)\n",
    "                    for eeg, character, latent, tokenid, embed in loaded_data:\n",
    "                        # 收集 latent 和 tokenid\n",
    "                        latents.append(latent.flatten())  # 将 latent 展平为 (64,)\n",
    "                        tokenids.append(tokenid[0])       # tokenid 是 int16\n",
    "                    pbar.update(1)\n",
    "\n",
    "# 转换为 NumPy 数组\n",
    "latents = np.array(latents)  # shape: (num_samples, 64)\n",
    "tokenids = np.array(tokenids)  # shape: (num_samples,)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-13T20:43:09.056154Z",
     "start_time": "2024-12-13T20:40:42.823844800Z"
    }
   },
   "id": "552d1884568b6a26"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-13T09:07:15.059874100Z",
     "start_time": "2024-12-13T09:07:15.047914100Z"
    }
   },
   "id": "7cc7851c6c90b1b7"
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "outputs": [],
   "source": [
    "# 使用 PCA 将数据降维到 32 维\n",
    "pca = PCA(n_components=2)\n",
    "latents_reduced = pca.fit_transform(latents)\n",
    "# 数据集划分\n",
    "X_train, X_test, y_train, y_test = train_test_split(latents_reduced, tokenids, test_size=0.1, random_state=42)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6de69a8939985ce2"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# 数据集划分\n",
    "X_train, X_test, y_train, y_test = train_test_split(latents, tokenids, test_size=0.1, random_state=42)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-13T19:14:08.768637Z",
     "start_time": "2024-12-13T19:14:08.525641700Z"
    }
   },
   "id": "9b82d1c87e2229d6"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k=5, 测试集准确率: 0.016413\n",
      "k=10, 测试集准确率: 0.018081\n",
      "k=50, 测试集准确率: 0.026045\n",
      "k=100, 测试集准确率: 0.028575\n",
      "k=1000, 测试集准确率: 0.032664\n",
      "最佳 k 值为: 1000, 对应的测试集准确率为: 0.032664\n",
      "所有 k 值和对应的准确率:\n",
      "k=5, 准确率: 0.016413\n",
      "k=10, 准确率: 0.018081\n",
      "k=50, 准确率: 0.026045\n",
      "k=100, 准确率: 0.028575\n",
      "k=1000, 准确率: 0.032664\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 遍历不同的 k 值，寻找最佳 k\n",
    "best_k = None\n",
    "best_accuracy = 0\n",
    "accuracy_results = []\n",
    "ks = [5,10,50,100,1000]\n",
    "\n",
    "for k in ks:  # 遍历 k 从 1 到 20\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)  # 创建 KNN 模型\n",
    "    knn.fit(X_train, y_train)  # 训练模型\n",
    "    accuracy = knn.score(X_test, y_test)  # 评估模型\n",
    "    accuracy_results.append((k, accuracy))  # 保存 k 和对应的准确率\n",
    "\n",
    "    if accuracy > best_accuracy:  # 如果当前 k 的准确率更高，更新最佳 k\n",
    "        best_k = k\n",
    "        best_accuracy = accuracy\n",
    "\n",
    "    print(f\"k={k}, 测试集准确率: {accuracy:.6f}\")\n",
    "\n",
    "# 输出最佳 k 和对应的准确率\n",
    "print(f\"最佳 k 值为: {best_k}, 对应的测试集准确率为: {best_accuracy:.6f}\")\n",
    "\n",
    "# 可选：打印所有 k 值和对应的准确率\n",
    "print(\"所有 k 值和对应的准确率:\")\n",
    "for k, acc in accuracy_results:\n",
    "    print(f\"k={k}, 准确率: {acc:.6f}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-13T19:17:03.596829400Z",
     "start_time": "2024-12-13T19:16:50.284416Z"
    }
   },
   "id": "7add9a93cfc69011"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练决策树:  25%|██▌       | 1/4 [01:36<04:48, 96.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_depth=5, 测试集准确率: 0.030619\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练决策树:  50%|█████     | 2/4 [04:46<05:03, 151.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_depth=10, 测试集准确率: 0.029274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练决策树:  75%|███████▌  | 3/4 [09:32<03:32, 212.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_depth=15, 测试集准确率: 0.029382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "训练决策树: 100%|██████████| 4/4 [15:51<00:00, 237.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_depth=20, 测试集准确率: 0.028521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 数据集划分\n",
    "X_train, X_test, y_train, y_test = train_test_split(latents, tokenids, test_size=0.1, random_state=42)\n",
    "\n",
    "# 超参数搜索的例子\n",
    "max_depths = [5, 10, 15, 20]\n",
    "for max_depth in tqdm(max_depths, desc=\"训练决策树\"):\n",
    "    tree_clf = DecisionTreeClassifier(max_depth=max_depth, random_state=42)\n",
    "    tree_clf.fit(X_train, y_train)\n",
    "    y_pred = tree_clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"max_depth={max_depth}, 测试集准确率: {accuracy:.6f}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-13T19:54:14.106517600Z",
     "start_time": "2024-12-13T19:38:22.605733800Z"
    }
   },
   "id": "f65a53309dcc7cbf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# 创建并训练随机森林回归模型\n",
    "rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_reg.fit(X_train, y_train)\n",
    "\n",
    "# 预测并计算 MSE 和 R²\n",
    "y_pred = rf_reg.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"随机森林回归测试集均方误差（MSE）: {mse:.6f}\")\n",
    "print(f\"随机森林回归测试集决定系数（R²）: {r2:.6f}\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-12-13T20:12:16.391855Z"
    }
   },
   "id": "cfbdbb5d79612cc0"
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# 定义 MLP 模型\n",
    "class LatentToEmbedModel(nn.Module):\n",
    "    def __init__(self, input_dim=64, hidden_dims=[512, 1024, 2048], output_dim=1456):\n",
    "        super(LatentToEmbedModel, self).__init__()\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "            layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(0.3))  # 防止过拟合\n",
    "            prev_dim = hidden_dim\n",
    "        layers.append(nn.Linear(prev_dim, output_dim))  # 输出层，维度为类别数\n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# 数据准备\n",
    "# 假设 latents 和 tokenids 已经是 NumPy 数组\n",
    "# latents: shape (num_samples, 64)\n",
    "# tokenids: shape (num_samples,)\n",
    "# tokenids 是 np.int64 类型，包含 1456 个不同的值\n",
    "\n",
    "# 获取 tokenids 的唯一值并创建映射\n",
    "unique_tokenids = np.unique(tokenids)  # 获取所有唯一的 tokenid 值\n",
    "print(len(unique_tokenids))\n",
    "id_to_class = {tokenid: idx for idx, tokenid in enumerate(unique_tokenids)}  # 原始值 -> 类别索引\n",
    "class_to_id = {idx: tokenid for tokenid, idx in id_to_class.items()}  # 类别索引 -> 原始值"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "371b7b3a5f8bb899"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1456\n",
      "Mapped tokenids min value: 0, max value: 1455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|██████████| 2614/2614 [00:43<00:00, 59.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 6.1771, Train Accuracy: 0.0309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|██████████| 2614/2614 [00:43<00:00, 60.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5.8725, Train Accuracy: 0.0321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|██████████| 2614/2614 [00:45<00:00, 57.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5.8417, Train Accuracy: 0.0324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|██████████| 2614/2614 [00:50<00:00, 51.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5.8257, Train Accuracy: 0.0325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|██████████| 2614/2614 [00:57<00:00, 45.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5.8143, Train Accuracy: 0.0348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|██████████| 2614/2614 [00:59<00:00, 44.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5.8015, Train Accuracy: 0.0347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|██████████| 2614/2614 [00:59<00:00, 44.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5.7949, Train Accuracy: 0.0363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|██████████| 2614/2614 [01:01<00:00, 42.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5.7874, Train Accuracy: 0.0363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|██████████| 2614/2614 [01:01<00:00, 42.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5.7810, Train Accuracy: 0.0362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|██████████| 2614/2614 [01:02<00:00, 41.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5.7770, Train Accuracy: 0.0366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|██████████| 2614/2614 [01:02<00:00, 41.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5.7722, Train Accuracy: 0.0365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|██████████| 2614/2614 [01:03<00:00, 41.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5.7683, Train Accuracy: 0.0370\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: 100%|██████████| 2614/2614 [01:05<00:00, 40.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5.7632, Train Accuracy: 0.0373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20: 100%|██████████| 2614/2614 [01:04<00:00, 40.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5.7615, Train Accuracy: 0.0368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: 100%|██████████| 2614/2614 [01:07<00:00, 38.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5.7572, Train Accuracy: 0.0370\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20: 100%|██████████| 2614/2614 [01:17<00:00, 33.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5.7563, Train Accuracy: 0.0374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20: 100%|██████████| 2614/2614 [01:07<00:00, 38.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5.7505, Train Accuracy: 0.0373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20: 100%|██████████| 2614/2614 [01:04<00:00, 40.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5.7489, Train Accuracy: 0.0376\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20: 100%|██████████| 2614/2614 [01:05<00:00, 40.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5.7460, Train Accuracy: 0.0372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20: 100%|██████████| 2614/2614 [01:05<00:00, 39.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5.7453, Train Accuracy: 0.0375\n",
      "Test Accuracy: 0.0264\n",
      "Predicted tokenids: [np.int64(1940), np.int64(1940), np.int64(1940), np.int64(1940), np.int64(35946), np.int64(35946), np.int64(1940), np.int64(1940), np.int64(1940), np.int64(35946), np.int64(9370), np.int64(1940), np.int64(1940), np.int64(1940), np.int64(1940), np.int64(35946), np.int64(35946), np.int64(1940), np.int64(1940), np.int64(1940), np.int64(35946), np.int64(1940), np.int64(35946)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 将 tokenids 映射为类别索引\n",
    "mapped_tokenids = np.array([id_to_class[tokenid] for tokenid in tokenids], dtype=np.int64)\n",
    "\n",
    "# 检查映射后的范围\n",
    "print(f\"Mapped tokenids min value: {mapped_tokenids.min()}, max value: {mapped_tokenids.max()}\")\n",
    "\n",
    "# 数据集划分\n",
    "X_train, X_test, y_train, y_test = train_test_split(latents, mapped_tokenids, test_size=0.1, random_state=42)\n",
    "\n",
    "# 转换为 PyTorch 张量\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)  # CrossEntropyLoss 需要 Long 类型标签\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# 创建数据加载器\n",
    "batch_size = 2048\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 模型、损失函数和优化器\n",
    "input_dim = X_train.shape[1]  # 输入维度\n",
    "hidden_dims = [512, 1024, 2048]\n",
    "output_dim = len(unique_tokenids)  # 输出类别数量\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = LatentToEmbedModel(input_dim=input_dim, hidden_dims=hidden_dims, output_dim=output_dim).to(device)\n",
    "criterion = nn.CrossEntropyLoss()  # 多分类问题使用交叉熵损失\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# 训练模型\n",
    "num_epochs = 20\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for X_batch, y_batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\"):\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        # 前向传播\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        \n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 统计训练损失和准确率\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += y_batch.size(0)\n",
    "        correct += (predicted == y_batch).sum().item()\n",
    "    \n",
    "    train_loss = running_loss / len(train_loader)\n",
    "    train_accuracy = correct / total\n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
    "\n",
    "# 测试模型\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        outputs = model(X_batch)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += y_batch.size(0)\n",
    "        correct += (predicted == y_batch).sum().item()\n",
    "\n",
    "test_accuracy = correct / total\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# 将预测结果映射回原始 tokenid\n",
    "predicted_tokenids = [class_to_id[idx] for idx in predicted.cpu().numpy()]\n",
    "print(f\"Predicted tokenids: {predicted_tokenids}\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-12-13T12:36:37.092547500Z"
    }
   },
   "id": "f6ba142ba5f4df04"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f11fd174cc3a8426"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 15\u001B[0m\n\u001B[0;32m     13\u001B[0m best_k \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m5\u001B[39m  \u001B[38;5;66;03m# 假设之前找到的最佳 k 值为 5\u001B[39;00m\n\u001B[0;32m     14\u001B[0m knn \u001B[38;5;241m=\u001B[39m KNeighborsClassifier(n_neighbors\u001B[38;5;241m=\u001B[39mbest_k)\n\u001B[1;32m---> 15\u001B[0m knn\u001B[38;5;241m.\u001B[39mfit(\u001B[43mX_train\u001B[49m, y_train)\n\u001B[0;32m     17\u001B[0m \u001B[38;5;66;03m# 2. 获取测试集的预测结果\u001B[39;00m\n\u001B[0;32m     18\u001B[0m y_pred \u001B[38;5;241m=\u001B[39m knn\u001B[38;5;241m.\u001B[39mpredict(X_test)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from matplotlib.colors import Normalize\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "# 假设以下变量已经存在：\n",
    "# X_train, y_train: 训练集特征和标签\n",
    "# X_test, y_test: 测试集特征和标签\n",
    "\n",
    "# 1. 使用最佳 k 值训练 KNN 模型\n",
    "best_k = 5  # 假设之前找到的最佳 k 值为 5\n",
    "knn = KNeighborsClassifier(n_neighbors=best_k)\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "# 2. 获取测试集的预测结果\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# 3. 使用 PCA 将测试集特征降维到 2 维\n",
    "pca = PCA(n_components=2)\n",
    "X_test_2d = pca.fit_transform(X_test)  # shape: (num_samples, 2)\n",
    "\n",
    "# 4. 定义颜色映射\n",
    "num_classes = len(np.unique(y_test))  # 类别总数\n",
    "norm = Normalize(vmin=0, vmax=num_classes - 1)  # 归一化类别索引范围\n",
    "cmap = cm.get_cmap('hsv', num_classes)  # 使用 HSV 色轮作为颜色映射\n",
    "\n",
    "# 5. 检测并移除离群点（使用分位数方法）\n",
    "# 计算每个点到数据中心的距离\n",
    "center = np.mean(X_test_2d, axis=0)  # 计算数据中心\n",
    "distances = np.linalg.norm(X_test_2d - center, axis=1)  # 计算欧几里得距离\n",
    "\n",
    "# 使用分位数定义距离阈值（例如，95% 分位数）\n",
    "distance_threshold = np.percentile(distances, 95)\n",
    "\n",
    "# 筛选出非离群点的索引\n",
    "inlier_mask = distances <= distance_threshold\n",
    "\n",
    "# 过滤掉离群点\n",
    "X_test_2d_filtered = X_test_2d[inlier_mask]\n",
    "y_test_filtered = y_test[inlier_mask]\n",
    "y_pred_filtered = y_pred[inlier_mask]\n",
    "\n",
    "# 6. 绘制过滤后的散点图\n",
    "plt.figure(figsize=(16, 8))\n",
    "\n",
    "# 左图：真实类别\n",
    "plt.subplot(1, 2, 1)\n",
    "sc1 = plt.scatter(\n",
    "    X_test_2d_filtered[:, 0],  # 第一主成分\n",
    "    X_test_2d_filtered[:, 1],  # 第二主成分\n",
    "    c=y_test_filtered,  # 类别索引\n",
    "    cmap=cmap,  # 使用颜色映射\n",
    "    norm=norm,  # 归一化\n",
    "    s=5,\n",
    "    alpha=0.7\n",
    ")\n",
    "plt.title(\"True Labels (PCA 2D, Outliers Removed)\", fontsize=16)\n",
    "plt.xlabel(\"Principal Component 1\", fontsize=14)\n",
    "plt.ylabel(\"Principal Component 2\", fontsize=14)\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "# 添加颜色条\n",
    "cbar1 = plt.colorbar(sc1, ticks=np.linspace(0, num_classes - 1, min(num_classes, 10)))  # 最多显示 10 个类别\n",
    "cbar1.set_label(\"Class Index\", fontsize=12)\n",
    "\n",
    "# # 右图：预测类别\n",
    "# plt.subplot(1, 2, 2)\n",
    "# sc2 = plt.scatter(\n",
    "#     X_test_2d_filtered[:, 0],  # 第一主成分\n",
    "#     X_test_2d_filtered[:, 1],  # 第二主成分\n",
    "#     c=y_pred_filtered,  # 类别索引\n",
    "#     cmap=cmap,  # 使用颜色映射\n",
    "#     norm=norm,  # 归一化\n",
    "#     s=5,\n",
    "#     alpha=0.7\n",
    "# )\n",
    "# plt.title(\"KNN Predictions (PCA 2D, Outliers Removed)\", fontsize=16)\n",
    "# plt.xlabel(\"Principal Component 1\", fontsize=14)\n",
    "# plt.ylabel(\"Principal Component 2\", fontsize=14)\n",
    "# plt.grid(alpha=0.3)\n",
    "# \n",
    "# # 添加颜色条\n",
    "# cbar2 = plt.colorbar(sc2, ticks=np.linspace(0, num_classes - 1, min(num_classes, 10)))  # 最多显示 10 个类别\n",
    "# cbar2.set_label(\"Class Index\", fontsize=12)\n",
    "plt.savefig(\"example_plot.pdf\", format=\"pdf\", bbox_inches=\"tight\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-13T18:04:02.895571100Z",
     "start_time": "2024-12-13T18:03:53.862120400Z"
    }
   },
   "id": "25e78ec926d59279"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['sub08', 'sub09', 'sub04', 'sub06', 'sub14', 'sub07', 'sub13', 'sub05'], ['sub10']), (['sub13', 'sub07', 'sub06', 'sub04', 'sub14', 'sub10', 'sub05', 'sub08'], ['sub09']), (['sub07', 'sub08', 'sub04', 'sub14', 'sub09', 'sub13', 'sub06', 'sub10'], ['sub05'])]\n",
      "\n",
      "Processing Split 1...\n",
      "Processing training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Subfolders:   0%|          | 0/8 [00:00<?, ?it/s]\n",
      "Files in sub08:   0%|          | 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "Files in sub08:  14%|█▍        | 1/7 [00:02<00:17,  2.84s/it]\u001B[A\n",
      "Files in sub08:  29%|██▊       | 2/7 [00:05<00:12,  2.60s/it]\u001B[A\n",
      "Files in sub08:  43%|████▎     | 3/7 [00:07<00:09,  2.38s/it]\u001B[A\n",
      "Files in sub08:  57%|█████▋    | 4/7 [00:10<00:07,  2.53s/it]\u001B[A\n",
      "Files in sub08:  71%|███████▏  | 5/7 [00:11<00:04,  2.03s/it]\u001B[A\n",
      "Files in sub08:  86%|████████▌ | 6/7 [00:13<00:02,  2.07s/it]\u001B[A\n",
      "Files in sub08: 100%|██████████| 7/7 [00:16<00:00,  2.24s/it]\u001B[A\n",
      "Training Subfolders:  12%|█▎        | 1/8 [00:16<01:52, 16.04s/it]\n",
      "Files in sub09:   0%|          | 0/4 [00:00<?, ?it/s]\u001B[A\n",
      "Files in sub09:  25%|██▌       | 1/4 [00:02<00:07,  2.45s/it]\u001B[A\n",
      "Files in sub09:  50%|█████     | 2/4 [00:03<00:03,  1.79s/it]\u001B[A\n",
      "Files in sub09:  75%|███████▌  | 3/4 [00:06<00:02,  2.02s/it]\u001B[A\n",
      "Files in sub09: 100%|██████████| 4/4 [00:08<00:00,  2.31s/it]\u001B[A\n",
      "Training Subfolders:  25%|██▌       | 2/8 [00:24<01:10, 11.80s/it]\n",
      "Files in sub04:   0%|          | 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "Files in sub04:  14%|█▍        | 1/7 [00:02<00:15,  2.57s/it]\u001B[A\n",
      "Files in sub04:  29%|██▊       | 2/7 [00:04<00:12,  2.45s/it]\u001B[A\n",
      "Files in sub04:  43%|████▎     | 3/7 [00:07<00:09,  2.31s/it]\u001B[A\n",
      "Files in sub04:  57%|█████▋    | 4/7 [00:09<00:07,  2.37s/it]\u001B[A\n",
      "Files in sub04:  71%|███████▏  | 5/7 [00:11<00:04,  2.05s/it]\u001B[A\n",
      "Files in sub04:  86%|████████▌ | 6/7 [00:13<00:02,  2.11s/it]\u001B[A\n",
      "Files in sub04: 100%|██████████| 7/7 [00:15<00:00,  2.31s/it]\u001B[A\n",
      "Training Subfolders:  38%|███▊      | 3/8 [00:40<01:08, 13.71s/it]\n",
      "Files in sub06:   0%|          | 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "Files in sub06:  14%|█▍        | 1/7 [00:02<00:15,  2.56s/it]\u001B[A\n",
      "Files in sub06:  29%|██▊       | 2/7 [00:04<00:11,  2.39s/it]\u001B[A\n",
      "Files in sub06:  43%|████▎     | 3/7 [00:06<00:09,  2.28s/it]\u001B[A\n",
      "Files in sub06:  57%|█████▋    | 4/7 [00:09<00:07,  2.34s/it]\u001B[A\n",
      "Files in sub06:  71%|███████▏  | 5/7 [00:10<00:03,  1.94s/it]\u001B[A\n",
      "Files in sub06:  86%|████████▌ | 6/7 [00:12<00:02,  2.02s/it]\u001B[A\n",
      "Files in sub06: 100%|██████████| 7/7 [00:15<00:00,  2.27s/it]\u001B[A\n",
      "Training Subfolders:  50%|█████     | 4/8 [00:56<00:57, 14.46s/it]\n",
      "Files in sub14:   0%|          | 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "Files in sub14:  14%|█▍        | 1/7 [00:02<00:17,  2.84s/it]\u001B[A\n",
      "Files in sub14:  29%|██▊       | 2/7 [00:05<00:13,  2.66s/it]\u001B[A\n",
      "Files in sub14:  43%|████▎     | 3/7 [00:07<00:09,  2.43s/it]\u001B[A\n",
      "Files in sub14:  57%|█████▋    | 4/7 [00:09<00:07,  2.44s/it]\u001B[A\n",
      "Files in sub14:  71%|███████▏  | 5/7 [00:11<00:04,  2.01s/it]\u001B[A\n",
      "Files in sub14:  86%|████████▌ | 6/7 [00:13<00:02,  2.07s/it]\u001B[A\n",
      "Files in sub14: 100%|██████████| 7/7 [00:16<00:00,  2.29s/it]\u001B[A\n",
      "Training Subfolders:  62%|██████▎   | 5/8 [01:12<00:45, 15.07s/it]\n",
      "Files in sub07:   0%|          | 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "Files in sub07:  14%|█▍        | 1/7 [00:02<00:15,  2.51s/it]\u001B[A\n",
      "Files in sub07:  29%|██▊       | 2/7 [00:04<00:11,  2.38s/it]\u001B[A\n",
      "Files in sub07:  43%|████▎     | 3/7 [00:06<00:08,  2.23s/it]\u001B[A\n",
      "Files in sub07:  57%|█████▋    | 4/7 [00:09<00:06,  2.33s/it]\u001B[A\n",
      "Files in sub07:  71%|███████▏  | 5/7 [00:10<00:03,  1.91s/it]\u001B[A\n",
      "Files in sub07:  86%|████████▌ | 6/7 [00:12<00:01,  2.00s/it]\u001B[A\n",
      "Files in sub07: 100%|██████████| 7/7 [00:15<00:00,  2.26s/it]\u001B[A\n",
      "Training Subfolders:  75%|███████▌  | 6/8 [01:28<00:30, 15.21s/it]\n",
      "Files in sub13:   0%|          | 0/5 [00:00<?, ?it/s]\u001B[A\n",
      "Files in sub13:  20%|██        | 1/5 [00:02<00:10,  2.53s/it]\u001B[A\n",
      "Files in sub13:  40%|████      | 2/5 [00:04<00:07,  2.40s/it]\u001B[A\n",
      "Files in sub13:  60%|██████    | 3/5 [00:06<00:04,  2.29s/it]\u001B[A\n",
      "Files in sub13:  80%|████████  | 4/5 [00:09<00:02,  2.31s/it]\u001B[A\n",
      "Files in sub13: 100%|██████████| 5/5 [00:12<00:00,  2.49s/it]\u001B[A\n",
      "Training Subfolders:  88%|████████▊ | 7/8 [01:40<00:14, 14.20s/it]\n",
      "Files in sub05:   0%|          | 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "Files in sub05:  14%|█▍        | 1/7 [00:02<00:15,  2.58s/it]\u001B[A\n",
      "Files in sub05:  29%|██▊       | 2/7 [00:05<00:12,  2.50s/it]\u001B[A\n",
      "Files in sub05:  43%|████▎     | 3/7 [00:07<00:09,  2.30s/it]\u001B[A\n",
      "Files in sub05:  57%|█████▋    | 4/7 [00:09<00:07,  2.41s/it]\u001B[A\n",
      "Files in sub05:  71%|███████▏  | 5/7 [00:10<00:03,  1.98s/it]\u001B[A\n",
      "Files in sub05:  86%|████████▌ | 6/7 [00:13<00:02,  2.09s/it]\u001B[A\n",
      "Files in sub05: 100%|██████████| 7/7 [00:15<00:00,  2.28s/it]\u001B[A\n",
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing testing data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Subfolders:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Files in sub10:   0%|          | 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "Files in sub10:  14%|█▍        | 1/7 [00:02<00:15,  2.62s/it]\u001B[A\n",
      "Files in sub10:  29%|██▊       | 2/7 [00:04<00:12,  2.44s/it]\u001B[A\n",
      "Files in sub10:  43%|████▎     | 3/7 [00:06<00:09,  2.26s/it]\u001B[A\n",
      "Files in sub10:  57%|█████▋    | 4/7 [00:09<00:07,  2.36s/it]\u001B[A\n",
      "Files in sub10:  71%|███████▏  | 5/7 [00:10<00:03,  1.95s/it]\u001B[A\n",
      "Files in sub10:  86%|████████▌ | 6/7 [00:13<00:02,  2.06s/it]\u001B[A\n",
      "Files in sub10: 100%|██████████| 7/7 [00:15<00:00,  2.26s/it]\u001B[A\n",
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Split 2...\n",
      "Processing training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Subfolders:   0%|          | 0/8 [00:00<?, ?it/s]\n",
      "Files in sub13:   0%|          | 0/5 [00:00<?, ?it/s]\u001B[A\n",
      "Files in sub13:  20%|██        | 1/5 [00:00<00:01,  2.19it/s]\u001B[A\n",
      "Files in sub13:  40%|████      | 2/5 [00:00<00:01,  2.47it/s]\u001B[A\n",
      "Files in sub13:  60%|██████    | 3/5 [00:01<00:01,  1.97it/s]\u001B[A\n",
      "Files in sub13:  80%|████████  | 4/5 [00:02<00:00,  1.85it/s]\u001B[A\n",
      "Files in sub13: 100%|██████████| 5/5 [00:02<00:00,  1.76it/s]\u001B[A\n",
      "Training Subfolders:  12%|█▎        | 1/8 [00:02<00:18,  2.67s/it]\n",
      "Files in sub07:   0%|          | 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "Files in sub07:  14%|█▍        | 1/7 [00:00<00:02,  2.61it/s]\u001B[A\n",
      "Files in sub07:  29%|██▊       | 2/7 [00:00<00:01,  2.50it/s]\u001B[A\n",
      "Files in sub07:  43%|████▎     | 3/7 [00:01<00:02,  1.97it/s]\u001B[A\n",
      "Files in sub07:  57%|█████▋    | 4/7 [00:02<00:01,  1.77it/s]\u001B[A\n",
      "Files in sub07:  71%|███████▏  | 5/7 [00:02<00:00,  2.00it/s]\u001B[A\n",
      "Files in sub07:  86%|████████▌ | 6/7 [00:03<00:00,  1.90it/s]\u001B[A\n",
      "Files in sub07: 100%|██████████| 7/7 [00:03<00:00,  1.83it/s]\u001B[A\n",
      "Training Subfolders:  25%|██▌       | 2/8 [00:06<00:19,  3.24s/it]\n",
      "Files in sub06:   0%|          | 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "Files in sub06:  14%|█▍        | 1/7 [00:02<00:15,  2.54s/it]\u001B[A\n",
      "Files in sub06:  29%|██▊       | 2/7 [00:04<00:12,  2.42s/it]\u001B[A\n",
      "Files in sub06:  43%|████▎     | 3/7 [00:06<00:09,  2.27s/it]\u001B[A\n",
      "Files in sub06:  57%|█████▋    | 4/7 [00:09<00:07,  2.35s/it]\u001B[A\n",
      "Files in sub06:  71%|███████▏  | 5/7 [00:10<00:03,  1.98s/it]\u001B[A\n",
      "Files in sub06:  86%|████████▌ | 6/7 [00:12<00:02,  2.06s/it]\u001B[A\n",
      "Files in sub06: 100%|██████████| 7/7 [00:15<00:00,  2.28s/it]\u001B[A\n",
      "Training Subfolders:  38%|███▊      | 3/8 [00:22<00:44,  8.94s/it]\n",
      "Files in sub04:   0%|          | 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "Files in sub04:  14%|█▍        | 1/7 [00:02<00:15,  2.54s/it]\u001B[A\n",
      "Files in sub04:  29%|██▊       | 2/7 [00:04<00:12,  2.46s/it]\u001B[A\n",
      "Files in sub04:  43%|████▎     | 3/7 [00:07<00:09,  2.32s/it]\u001B[A\n",
      "Files in sub04:  57%|█████▋    | 4/7 [00:09<00:07,  2.41s/it]\u001B[A\n",
      "Files in sub04:  71%|███████▏  | 5/7 [00:10<00:04,  2.01s/it]\u001B[A\n",
      "Files in sub04:  86%|████████▌ | 6/7 [00:13<00:02,  2.09s/it]\u001B[A\n",
      "Files in sub04: 100%|██████████| 7/7 [00:16<00:00,  2.33s/it]\u001B[A\n",
      "Training Subfolders:  50%|█████     | 4/8 [00:38<00:46, 11.73s/it]\n",
      "Files in sub14:   0%|          | 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "Files in sub14:  14%|█▍        | 1/7 [00:02<00:15,  2.57s/it]\u001B[A\n",
      "Files in sub14:  29%|██▊       | 2/7 [00:04<00:12,  2.46s/it]\u001B[A\n",
      "Files in sub14:  43%|████▎     | 3/7 [00:07<00:09,  2.33s/it]\u001B[A\n",
      "Files in sub14:  57%|█████▋    | 4/7 [00:09<00:07,  2.49s/it]\u001B[A\n",
      "Files in sub14:  71%|███████▏  | 5/7 [00:11<00:04,  2.06s/it]\u001B[A\n",
      "Files in sub14:  86%|████████▌ | 6/7 [00:13<00:02,  2.13s/it]\u001B[A\n",
      "Files in sub14: 100%|██████████| 7/7 [00:16<00:00,  2.38s/it]\u001B[A\n",
      "Training Subfolders:  62%|██████▎   | 5/8 [00:54<00:40, 13.39s/it]\n",
      "Files in sub10:   0%|          | 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "Files in sub10:  14%|█▍        | 1/7 [00:02<00:17,  2.92s/it]\u001B[A\n",
      "Files in sub10:  29%|██▊       | 2/7 [00:05<00:12,  2.57s/it]\u001B[A\n",
      "Files in sub10:  43%|████▎     | 3/7 [00:07<00:09,  2.45s/it]\u001B[A\n",
      "Files in sub10:  57%|█████▋    | 4/7 [00:10<00:07,  2.50s/it]\u001B[A\n",
      "Files in sub10:  71%|███████▏  | 5/7 [00:11<00:04,  2.07s/it]\u001B[A\n",
      "Files in sub10:  86%|████████▌ | 6/7 [00:13<00:02,  2.14s/it]\u001B[A\n",
      "Files in sub10: 100%|██████████| 7/7 [00:16<00:00,  2.34s/it]\u001B[A\n",
      "Training Subfolders:  75%|███████▌  | 6/8 [01:10<00:28, 14.43s/it]\n",
      "Files in sub05:   0%|          | 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "Files in sub05:  14%|█▍        | 1/7 [00:02<00:17,  2.92s/it]\u001B[A\n",
      "Files in sub05:  29%|██▊       | 2/7 [00:05<00:12,  2.59s/it]\u001B[A\n",
      "Files in sub05:  43%|████▎     | 3/7 [00:07<00:09,  2.42s/it]\u001B[A\n",
      "Files in sub05:  57%|█████▋    | 4/7 [00:10<00:07,  2.49s/it]\u001B[A\n",
      "Files in sub05:  71%|███████▏  | 5/7 [00:11<00:04,  2.09s/it]\u001B[A\n",
      "Files in sub05:  86%|████████▌ | 6/7 [00:13<00:02,  2.16s/it]\u001B[A\n",
      "Files in sub05: 100%|██████████| 7/7 [00:16<00:00,  2.40s/it]\u001B[A\n",
      "Training Subfolders:  88%|████████▊ | 7/8 [01:27<00:15, 15.17s/it]\n",
      "Files in sub08:   0%|          | 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "Files in sub08:  14%|█▍        | 1/7 [00:02<00:15,  2.57s/it]\u001B[A\n",
      "Files in sub08:  29%|██▊       | 2/7 [00:04<00:12,  2.45s/it]\u001B[A\n",
      "Files in sub08:  43%|████▎     | 3/7 [00:07<00:09,  2.31s/it]\u001B[A\n",
      "Files in sub08:  57%|█████▋    | 4/7 [00:09<00:07,  2.42s/it]\u001B[A\n",
      "Files in sub08:  71%|███████▏  | 5/7 [00:11<00:04,  2.04s/it]\u001B[A\n",
      "Files in sub08:  86%|████████▌ | 6/7 [00:13<00:02,  2.11s/it]\u001B[A\n",
      "Files in sub08: 100%|██████████| 7/7 [00:16<00:00,  2.48s/it]\u001B[A\n",
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing testing data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Subfolders:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Files in sub09:   0%|          | 0/4 [00:00<?, ?it/s]\u001B[A\n",
      "Files in sub09:  25%|██▌       | 1/4 [00:02<00:08,  2.76s/it]\u001B[A\n",
      "Files in sub09:  50%|█████     | 2/4 [00:04<00:03,  1.98s/it]\u001B[A\n",
      "Files in sub09:  75%|███████▌  | 3/4 [00:06<00:02,  2.14s/it]\u001B[A\n",
      "Files in sub09: 100%|██████████| 4/4 [00:09<00:00,  2.42s/it]\u001B[A\n",
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing Split 3...\n",
      "Processing training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Subfolders:   0%|          | 0/8 [00:00<?, ?it/s]\n",
      "Files in sub07:   0%|          | 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "Files in sub07:  14%|█▍        | 1/7 [00:02<00:15,  2.51s/it]\u001B[A\n",
      "Files in sub07:  29%|██▊       | 2/7 [00:05<00:13,  2.62s/it]\u001B[A\n",
      "Files in sub07:  43%|████▎     | 3/7 [00:07<00:09,  2.42s/it]\u001B[A\n",
      "Files in sub07:  57%|█████▋    | 4/7 [00:10<00:07,  2.51s/it]\u001B[A\n",
      "Files in sub07:  71%|███████▏  | 5/7 [00:11<00:04,  2.10s/it]\u001B[A\n",
      "Files in sub07:  86%|████████▌ | 6/7 [00:13<00:02,  2.15s/it]\u001B[A\n",
      "Files in sub07: 100%|██████████| 7/7 [00:16<00:00,  2.37s/it]\u001B[A\n",
      "Training Subfolders:  12%|█▎        | 1/8 [00:16<01:55, 16.49s/it]\n",
      "Files in sub08:   0%|          | 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "Files in sub08:  14%|█▍        | 1/7 [00:00<00:03,  1.94it/s]\u001B[A\n",
      "Files in sub08:  29%|██▊       | 2/7 [00:00<00:02,  2.12it/s]\u001B[A\n",
      "Files in sub08:  43%|████▎     | 3/7 [00:01<00:02,  1.38it/s]\u001B[A\n",
      "Files in sub08:  57%|█████▋    | 4/7 [00:02<00:02,  1.23it/s]\u001B[A\n",
      "Files in sub08:  71%|███████▏  | 5/7 [00:03<00:01,  1.31it/s]\u001B[A\n",
      "Files in sub08:  86%|████████▌ | 6/7 [00:04<00:00,  1.25it/s]\u001B[A\n",
      "Files in sub08: 100%|██████████| 7/7 [00:05<00:00,  1.25it/s]\u001B[A\n",
      "Training Subfolders:  25%|██▌       | 2/8 [00:21<00:59,  9.90s/it]\n",
      "Files in sub04:   0%|          | 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "Files in sub04:  14%|█▍        | 1/7 [00:02<00:15,  2.57s/it]\u001B[A\n",
      "Files in sub04:  29%|██▊       | 2/7 [00:05<00:12,  2.50s/it]\u001B[A\n",
      "Files in sub04:  43%|████▎     | 3/7 [00:07<00:09,  2.41s/it]\u001B[A\n",
      "Files in sub04:  57%|█████▋    | 4/7 [00:10<00:07,  2.52s/it]\u001B[A\n",
      "Files in sub04:  71%|███████▏  | 5/7 [00:11<00:04,  2.12s/it]\u001B[A\n",
      "Files in sub04:  86%|████████▌ | 6/7 [00:13<00:02,  2.19s/it]\u001B[A\n",
      "Files in sub04: 100%|██████████| 7/7 [00:16<00:00,  2.42s/it]\u001B[A\n",
      "Training Subfolders:  38%|███▊      | 3/8 [00:38<01:04, 12.98s/it]\n",
      "Files in sub14:   0%|          | 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "Files in sub14:  14%|█▍        | 1/7 [00:02<00:15,  2.62s/it]\u001B[A\n",
      "Files in sub14:  29%|██▊       | 2/7 [00:05<00:12,  2.48s/it]\u001B[A\n",
      "Files in sub14:  43%|████▎     | 3/7 [00:07<00:09,  2.36s/it]\u001B[A\n",
      "Files in sub14:  57%|█████▋    | 4/7 [00:09<00:07,  2.50s/it]\u001B[A\n",
      "Files in sub14:  71%|███████▏  | 5/7 [00:11<00:04,  2.11s/it]\u001B[A\n",
      "Files in sub14:  86%|████████▌ | 6/7 [00:13<00:02,  2.22s/it]\u001B[A\n",
      "Files in sub14: 100%|██████████| 7/7 [00:16<00:00,  2.43s/it]\u001B[A\n",
      "Training Subfolders:  50%|█████     | 4/8 [00:55<00:57, 14.44s/it]\n",
      "Files in sub09:   0%|          | 0/4 [00:00<?, ?it/s]\u001B[A\n",
      "Files in sub09:  25%|██▌       | 1/4 [00:02<00:08,  2.80s/it]\u001B[A\n",
      "Files in sub09:  50%|█████     | 2/4 [00:04<00:03,  1.95s/it]\u001B[A\n",
      "Files in sub09:  75%|███████▌  | 3/4 [00:06<00:02,  2.12s/it]\u001B[A\n",
      "Files in sub09: 100%|██████████| 4/4 [00:09<00:00,  2.41s/it]\u001B[A\n",
      "Training Subfolders:  62%|██████▎   | 5/8 [01:04<00:37, 12.60s/it]\n",
      "Files in sub13:   0%|          | 0/5 [00:00<?, ?it/s]\u001B[A\n",
      "Files in sub13:  20%|██        | 1/5 [00:02<00:10,  2.67s/it]\u001B[A\n",
      "Files in sub13:  40%|████      | 2/5 [00:05<00:07,  2.56s/it]\u001B[A\n",
      "Files in sub13:  60%|██████    | 3/5 [00:07<00:04,  2.43s/it]\u001B[A\n",
      "Files in sub13:  80%|████████  | 4/5 [00:09<00:02,  2.46s/it]\u001B[A\n",
      "Files in sub13: 100%|██████████| 5/5 [00:12<00:00,  2.63s/it]\u001B[A\n",
      "Training Subfolders:  75%|███████▌  | 6/8 [01:17<00:25, 12.69s/it]\n",
      "Files in sub06:   0%|          | 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "Files in sub06:  14%|█▍        | 1/7 [00:02<00:15,  2.61s/it]\u001B[A\n",
      "Files in sub06:  29%|██▊       | 2/7 [00:05<00:12,  2.50s/it]\u001B[A\n",
      "Files in sub06:  43%|████▎     | 3/7 [00:07<00:09,  2.40s/it]\u001B[A\n",
      "Files in sub06:  57%|█████▋    | 4/7 [00:09<00:07,  2.51s/it]\u001B[A\n",
      "Files in sub06:  71%|███████▏  | 5/7 [00:11<00:04,  2.13s/it]\u001B[A\n",
      "Files in sub06:  86%|████████▌ | 6/7 [00:13<00:02,  2.18s/it]\u001B[A\n",
      "Files in sub06: 100%|██████████| 7/7 [00:16<00:00,  2.43s/it]\u001B[A\n",
      "Training Subfolders:  88%|████████▊ | 7/8 [01:34<00:14, 14.00s/it]\n",
      "Files in sub10:   0%|          | 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "Files in sub10:  14%|█▍        | 1/7 [00:02<00:16,  2.76s/it]\u001B[A\n",
      "Files in sub10:  29%|██▊       | 2/7 [00:05<00:12,  2.53s/it]\u001B[A\n",
      "Files in sub10:  43%|████▎     | 3/7 [00:07<00:09,  2.39s/it]\u001B[A\n",
      "Files in sub10:  57%|█████▋    | 4/7 [00:09<00:07,  2.49s/it]\u001B[A\n",
      "Files in sub10:  71%|███████▏  | 5/7 [00:11<00:04,  2.11s/it]\u001B[A\n",
      "Files in sub10:  86%|████████▌ | 6/7 [00:13<00:02,  2.23s/it]\u001B[A\n",
      "Files in sub10: 100%|██████████| 7/7 [00:16<00:00,  2.45s/it]\u001B[A\n",
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing testing data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Subfolders:   0%|          | 0/1 [00:00<?, ?it/s]\n",
      "Files in sub05:   0%|          | 0/7 [00:00<?, ?it/s]\u001B[A\n",
      "Files in sub05:  14%|█▍        | 1/7 [00:02<00:15,  2.64s/it]\u001B[A\n",
      "Files in sub05:  29%|██▊       | 2/7 [00:05<00:12,  2.52s/it]\u001B[A\n",
      "Files in sub05:  43%|████▎     | 3/7 [00:07<00:09,  2.42s/it]\u001B[A\n",
      "Files in sub05:  57%|█████▋    | 4/7 [00:10<00:07,  2.55s/it]\u001B[A\n",
      "Files in sub05:  71%|███████▏  | 5/7 [00:11<00:04,  2.16s/it]\u001B[A\n",
      "Files in sub05:  86%|████████▌ | 6/7 [00:13<00:02,  2.22s/it]\u001B[A\n",
      "Files in sub05: 100%|██████████| 7/7 [00:17<00:00,  2.50s/it]\u001B[A\n",
      "                                                                 \r"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "# 数据路径\n",
    "root_dir = 'D:\\\\PythonProject\\\\DLA_ML_project\\\\sh-DLAndML-project\\\\Data\\\\qwen-characterSplit'\n",
    "\n",
    "# 无效的子文件夹\n",
    "invalid_subs = {'sub11', 'sub12', 'sub15'}\n",
    "\n",
    "# 获取有效的子文件夹\n",
    "sub_dirs = [sub_dir for sub_dir in os.listdir(root_dir) \n",
    "            if os.path.isdir(os.path.join(root_dir, sub_dir)) and sub_dir not in invalid_subs]\n",
    "\n",
    "# 确保每次运行生成的分组一致（可选）\n",
    "random.seed(44)\n",
    "\n",
    "# 生成三组训练集和测试集\n",
    "splits = []\n",
    "for _ in range(3):\n",
    "    random.shuffle(sub_dirs)\n",
    "    train_subs = sub_dirs[:8]\n",
    "    test_subs = sub_dirs[8:]\n",
    "    splits.append((train_subs, test_subs))\n",
    "\n",
    "print(splits)\n",
    "\n",
    "# 存储三组数据\n",
    "all_data = []\n",
    "\n",
    "for split_idx, (train_subs, test_subs) in enumerate(splits):\n",
    "    print(f\"\\nProcessing Split {split_idx + 1}...\")\n",
    "\n",
    "    # 初始化存储\n",
    "    train_latents, train_tokenids = [], []\n",
    "    test_latents, test_tokenids = [], []\n",
    "\n",
    "    # 处理训练集\n",
    "    print(\"Processing training data...\")\n",
    "    for sub_dir in tqdm(train_subs, desc=\"Training Subfolders\", leave=False):\n",
    "        sub_dir_path = os.path.join(root_dir, sub_dir)\n",
    "        for file_name in tqdm(os.listdir(sub_dir_path), desc=f\"Files in {sub_dir}\", leave=False):\n",
    "            if file_name.endswith('.pkl') and 'latent' in file_name and '_tokenid' in file_name and '_tokenEmbedding' in file_name:\n",
    "                file_path = os.path.join(sub_dir_path, file_name)\n",
    "                with open(file_path, 'rb') as file:\n",
    "                    loaded_data = pickle.load(file)\n",
    "                for eeg, character, latent, tokenid, embed in loaded_data:\n",
    "                    train_latents.append(latent.flatten())  # 将 latent 展平为 (64,)\n",
    "                    train_tokenids.append(tokenid[0])       # tokenid 是 int16\n",
    "\n",
    "    # 处理测试集\n",
    "    print(\"Processing testing data...\")\n",
    "    for sub_dir in tqdm(test_subs, desc=\"Testing Subfolders\", leave=False):\n",
    "        sub_dir_path = os.path.join(root_dir, sub_dir)\n",
    "        for file_name in tqdm(os.listdir(sub_dir_path), desc=f\"Files in {sub_dir}\", leave=False):\n",
    "            if file_name.endswith('.pkl') and 'latent' in file_name and '_tokenid' in file_name and '_tokenEmbedding' in file_name:\n",
    "                file_path = os.path.join(sub_dir_path, file_name)\n",
    "                with open(file_path, 'rb') as file:\n",
    "                    loaded_data = pickle.load(file)\n",
    "                for eeg, character, latent, tokenid, embed in loaded_data:\n",
    "                    test_latents.append(latent.flatten())  # 将 latent 展平为 (64,)\n",
    "                    test_tokenids.append(tokenid[0])       # tokenid 是 int16\n",
    "\n",
    "    # 转换为 NumPy 数组\n",
    "    train_latents = np.array(train_latents)\n",
    "    train_tokenids = np.array(train_tokenids)\n",
    "    test_latents = np.array(test_latents)\n",
    "    test_tokenids = np.array(test_tokenids)\n",
    "\n",
    "    # 保存到 all_data\n",
    "    all_data.append({\n",
    "        \"train_latents\": train_latents,\n",
    "        \"train_tokenids\": train_tokenids,\n",
    "        \"test_latents\": test_latents,\n",
    "        \"test_tokenids\": test_tokenids\n",
    "    })\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-13T21:02:13.577638100Z",
     "start_time": "2024-12-13T20:56:00.195963700Z"
    }
   },
   "id": "80742c707a4fdb87"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique tokenids: 1456\n"
     ]
    }
   ],
   "source": [
    "# 获取所有唯一的 tokenid 并创建映射\n",
    "unique_tokenids = np.unique(tokenids)  # 获取所有唯一的 tokenid 值\n",
    "print(f\"Total unique tokenids: {len(unique_tokenids)}\")\n",
    "\n",
    "id_to_class = {tokenid: idx for idx, tokenid in enumerate(unique_tokenids)}  # 原始值 -> 类别索引\n",
    "class_to_id = {idx: tokenid for tokenid, idx in id_to_class.items()}  # 类别索引 -> 原始值"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-13T21:24:47.283775300Z",
     "start_time": "2024-12-13T21:24:47.220470200Z"
    }
   },
   "id": "b3fcda378c22cffe"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "np.int64(1)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 29\u001B[0m\n\u001B[0;32m     27\u001B[0m \u001B[38;5;66;03m# 更新 all_data 中的 tokenid 为映射后的类别索引\u001B[39;00m\n\u001B[0;32m     28\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m split_data \u001B[38;5;129;01min\u001B[39;00m all_data:\n\u001B[1;32m---> 29\u001B[0m     split_data[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain_tokenids\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray([id_to_class[tokenid] \u001B[38;5;28;01mfor\u001B[39;00m tokenid \u001B[38;5;129;01min\u001B[39;00m split_data[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain_tokenids\u001B[39m\u001B[38;5;124m\"\u001B[39m]], dtype\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39mint64)\n\u001B[0;32m     30\u001B[0m     split_data[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest_tokenids\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray([id_to_class[tokenid] \u001B[38;5;28;01mfor\u001B[39;00m tokenid \u001B[38;5;129;01min\u001B[39;00m split_data[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest_tokenids\u001B[39m\u001B[38;5;124m\"\u001B[39m]], dtype\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39mint64)\n",
      "Cell \u001B[1;32mIn[7], line 29\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     27\u001B[0m \u001B[38;5;66;03m# 更新 all_data 中的 tokenid 为映射后的类别索引\u001B[39;00m\n\u001B[0;32m     28\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m split_data \u001B[38;5;129;01min\u001B[39;00m all_data:\n\u001B[1;32m---> 29\u001B[0m     split_data[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain_tokenids\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray([\u001B[43mid_to_class\u001B[49m\u001B[43m[\u001B[49m\u001B[43mtokenid\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m tokenid \u001B[38;5;129;01min\u001B[39;00m split_data[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain_tokenids\u001B[39m\u001B[38;5;124m\"\u001B[39m]], dtype\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39mint64)\n\u001B[0;32m     30\u001B[0m     split_data[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest_tokenids\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray([id_to_class[tokenid] \u001B[38;5;28;01mfor\u001B[39;00m tokenid \u001B[38;5;129;01min\u001B[39;00m split_data[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest_tokenids\u001B[39m\u001B[38;5;124m\"\u001B[39m]], dtype\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39mint64)\n",
      "\u001B[1;31mKeyError\u001B[0m: np.int64(1)"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "# 定义 MLP 模型\n",
    "class LatentToTokenidModel(nn.Module):\n",
    "    def __init__(self, input_dim=64, hidden_dims=[512, 1024, 2048], output_dim=1456):\n",
    "        super(LatentToTokenidModel, self).__init__()\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "            layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(0.3))  # 防止过拟合\n",
    "            prev_dim = hidden_dim\n",
    "        layers.append(nn.Linear(prev_dim, output_dim))  # 输出层，维度为类别数\n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# 更新 all_data 中的 tokenid 为映射后的类别索引\n",
    "for split_data in all_data:\n",
    "    split_data[\"train_tokenids\"] = np.array([id_to_class[tokenid] for tokenid in split_data[\"train_tokenids\"]], dtype=np.int64)\n",
    "    split_data[\"test_tokenids\"] = np.array([id_to_class[tokenid] for tokenid in split_data[\"test_tokenids\"]], dtype=np.int64)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-13T21:25:01.885722600Z",
     "start_time": "2024-12-13T21:25:01.689007300Z"
    }
   },
   "id": "247080591a6242a2"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "np.int64(1)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyError\u001B[0m                                  Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[13], line 29\u001B[0m\n\u001B[0;32m     27\u001B[0m \u001B[38;5;66;03m# 更新 all_data 中的 tokenid 为映射后的类别索引\u001B[39;00m\n\u001B[0;32m     28\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m split_data \u001B[38;5;129;01min\u001B[39;00m all_data:\n\u001B[1;32m---> 29\u001B[0m     split_data[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain_tokenids\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray([id_to_class[tokenid] \u001B[38;5;28;01mfor\u001B[39;00m tokenid \u001B[38;5;129;01min\u001B[39;00m split_data[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain_tokenids\u001B[39m\u001B[38;5;124m\"\u001B[39m]], dtype\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39mint64)\n\u001B[0;32m     30\u001B[0m     split_data[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest_tokenids\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray([id_to_class[tokenid] \u001B[38;5;28;01mfor\u001B[39;00m tokenid \u001B[38;5;129;01min\u001B[39;00m split_data[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest_tokenids\u001B[39m\u001B[38;5;124m\"\u001B[39m]], dtype\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39mint64)\n\u001B[0;32m     33\u001B[0m \u001B[38;5;66;03m# 模型、损失函数和优化器设置\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[13], line 29\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m     27\u001B[0m \u001B[38;5;66;03m# 更新 all_data 中的 tokenid 为映射后的类别索引\u001B[39;00m\n\u001B[0;32m     28\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m split_data \u001B[38;5;129;01min\u001B[39;00m all_data:\n\u001B[1;32m---> 29\u001B[0m     split_data[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain_tokenids\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray([\u001B[43mid_to_class\u001B[49m\u001B[43m[\u001B[49m\u001B[43mtokenid\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m tokenid \u001B[38;5;129;01min\u001B[39;00m split_data[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrain_tokenids\u001B[39m\u001B[38;5;124m\"\u001B[39m]], dtype\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39mint64)\n\u001B[0;32m     30\u001B[0m     split_data[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest_tokenids\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39marray([id_to_class[tokenid] \u001B[38;5;28;01mfor\u001B[39;00m tokenid \u001B[38;5;129;01min\u001B[39;00m split_data[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest_tokenids\u001B[39m\u001B[38;5;124m\"\u001B[39m]], dtype\u001B[38;5;241m=\u001B[39mnp\u001B[38;5;241m.\u001B[39mint64)\n\u001B[0;32m     33\u001B[0m \u001B[38;5;66;03m# 模型、损失函数和优化器设置\u001B[39;00m\n",
      "\u001B[1;31mKeyError\u001B[0m: np.int64(1)"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 模型、损失函数和优化器设置\n",
    "input_dim = 64  # 输入维度\n",
    "hidden_dims = [512, 1024, 2048]\n",
    "output_dim = 1456  # 输出类别数量\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# 训练、验证和测试\n",
    "num_epochs = 200\n",
    "batch_size = 2048\n",
    "\n",
    "for split_idx, split_data in enumerate(all_data):\n",
    "    print(f\"\\nProcessing Split {split_idx + 1}...\")\n",
    "\n",
    "    # 从 all_data 中获取训练集、验证集和测试集\n",
    "    train_latents = split_data[\"train_latents\"]\n",
    "    train_tokenids = split_data[\"train_tokenids\"]\n",
    "    test_latents = split_data[\"test_latents\"]\n",
    "    test_tokenids = split_data[\"test_tokenids\"]\n",
    "\n",
    "    # 划分验证集（从训练集中划分 10%）\n",
    "    val_size = int(0.1 * len(train_latents))\n",
    "    val_latents = train_latents[:val_size]\n",
    "    val_tokenids = train_tokenids[:val_size]\n",
    "    train_latents = train_latents[val_size:]\n",
    "    train_tokenids = train_tokenids[val_size:]\n",
    "\n",
    "    # 转换为 PyTorch 张量\n",
    "    train_dataset = TensorDataset(torch.tensor(train_latents, dtype=torch.float32),\n",
    "                                   torch.tensor(train_tokenids, dtype=torch.long))\n",
    "    val_dataset = TensorDataset(torch.tensor(val_latents, dtype=torch.float32),\n",
    "                                 torch.tensor(val_tokenids, dtype=torch.long))\n",
    "    test_dataset = TensorDataset(torch.tensor(test_latents, dtype=torch.float32),\n",
    "                                  torch.tensor(test_tokenids, dtype=torch.long))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # 初始化模型、损失函数和优化器\n",
    "    model = LatentToTokenidModel(input_dim=input_dim, hidden_dims=hidden_dims, output_dim=output_dim).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "    # 初始化绘图\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.title(f\"Loss Curve for Split {split_idx + 1}\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.ion()  # 开启交互模式\n",
    "\n",
    "    # 训练模型\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        for X_batch, y_batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} (Train)\", leave=False):\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            # 前向传播\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "\n",
    "            # 反向传播和优化\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # 统计训练损失\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # 验证模型\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        # 更新实时绘图\n",
    "        plt.clf()  # 清除当前图像\n",
    "        plt.plot(range(1, len(train_losses) + 1), train_losses, label=\"Train Loss\", marker='o')\n",
    "        plt.plot(range(1, len(val_losses) + 1), val_losses, label=\"Validation Loss\", marker='o')\n",
    "        plt.legend()\n",
    "        plt.pause(0.1)  # 暂停以更新图像\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "        # 保存模型\n",
    "        torch.save(model.state_dict(), f\"model_split_{split_idx + 1}_epoch_{epoch + 1}.pth\")\n",
    "\n",
    "    # 测试模型\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += y_batch.size(0)\n",
    "            correct += (predicted == y_batch).sum().item()\n",
    "\n",
    "    test_accuracy = correct / total\n",
    "    print(f\"Test Accuracy for Split {split_idx + 1}: {test_accuracy:.4f}\")\n",
    "\n",
    "    # 保存最终的 Loss 曲线\n",
    "    plt.ioff()  # 关闭交互模式\n",
    "    plt.savefig(f\"loss_curve_split_{split_idx + 1}.png\")\n",
    "    plt.close()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-12-13T18:37:06.794515800Z"
    }
   },
   "id": "12d377d655bce4f5"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "\n",
      "Processing Split 1...\n",
      "\n",
      "Processing Split 2...\n",
      "\n",
      "Processing Split 3...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# 定义 MLP 模型\n",
    "class LatentToTokenidModel(nn.Module):\n",
    "    def __init__(self, input_dim=64, hidden_dims=[512, 1024, 2048], output_dim=1456):\n",
    "        super(LatentToTokenidModel, self).__init__()\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "            layers.append(nn.BatchNorm1d(hidden_dim))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(0.3))  # 防止过拟合\n",
    "            prev_dim = hidden_dim\n",
    "        layers.append(nn.Linear(prev_dim, output_dim))  # 输出层，维度为类别数\n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "save_dir = \"./parameter\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "# 模型、损失函数和优化器设置\n",
    "input_dim = 64  # 输入维度\n",
    "hidden_dims = [512, 1024, 2048]\n",
    "output_dim = 1456  # 输出类别数量\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# 训练、验证和测试\n",
    "num_epochs = 200\n",
    "batch_size = 2048\n",
    "\n",
    "for split_idx, split_data in enumerate(all_data):\n",
    "    print(f\"\\nProcessing Split {split_idx + 1}...\")\n",
    "\n",
    "    # 从 all_data 中获取训练集、验证集和测试集\n",
    "    train_latents = split_data[\"train_latents\"]\n",
    "    train_tokenids = split_data[\"train_tokenids\"]\n",
    "    test_latents = split_data[\"test_latents\"]\n",
    "    test_tokenids = split_data[\"test_tokenids\"]\n",
    "\n",
    "    # 划分验证集（从训练集中划分 10%）\n",
    "    val_size = int(0.1 * len(train_latents))\n",
    "    val_latents = train_latents[:val_size]\n",
    "    val_tokenids = train_tokenids[:val_size]\n",
    "    train_latents = train_latents[val_size:]\n",
    "    train_tokenids = train_tokenids[val_size:]\n",
    "\n",
    "    # 转换为 PyTorch 张量\n",
    "    train_dataset = TensorDataset(torch.tensor(train_latents, dtype=torch.float32),\n",
    "                                   torch.tensor(train_tokenids, dtype=torch.long))\n",
    "    val_dataset = TensorDataset(torch.tensor(val_latents, dtype=torch.float32),\n",
    "                                 torch.tensor(val_tokenids, dtype=torch.long))\n",
    "    test_dataset = TensorDataset(torch.tensor(test_latents, dtype=torch.float32),\n",
    "                                  torch.tensor(test_tokenids, dtype=torch.long))\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # 初始化模型、损失函数和优化器\n",
    "    model = LatentToTokenidModel(input_dim=input_dim, hidden_dims=hidden_dims, output_dim=output_dim).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-13T21:26:05.625934800Z",
     "start_time": "2024-12-13T21:26:02.324673800Z"
    }
   },
   "id": "c77bcdf70f546a58"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "    # 初始化绘图\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.title(f\"Loss Curve for Split {split_idx + 1}\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.ion()  # 开启交互模式\n",
    "\n",
    "    # 训练模型\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        for X_batch, y_batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} (Train)\", leave=False):\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "            # 前向传播\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "\n",
    "            # 反向传播和优化\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # 统计训练损失和准确率\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_train += y_batch.size(0)\n",
    "            correct_train += (predicted == y_batch).sum().item()\n",
    "\n",
    "        train_loss = running_loss / len(train_loader)\n",
    "        train_accuracy = correct_train / total_train\n",
    "        train_losses.append(train_loss)\n",
    "\n",
    "        # 验证模型\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        correct_val = 0\n",
    "        total_val = 0\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total_val += y_batch.size(0)\n",
    "                correct_val += (predicted == y_batch).sum().item()\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        val_accuracy = correct_val / total_val\n",
    "        val_losses.append(val_loss)\n",
    "\n",
    "        # 更新实时绘图\n",
    "        plt.clf()  # 清除当前图像\n",
    "        plt.plot(range(1, len(train_losses) + 1), train_losses, label=\"Train Loss\", marker='o')\n",
    "        plt.plot(range(1, len(val_losses) + 1), val_losses, label=\"Validation Loss\", marker='o')\n",
    "        plt.legend()\n",
    "        plt.pause(0.1)  # 暂停以更新图像\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \"\n",
    "              f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "        # 保存模型\n",
    "        model_save_path = os.path.join(save_dir, f\"model_split_{split_idx + 1}_epoch_{epoch + 1}.pth\")\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "\n",
    "    # 测试模型\n",
    "    model.eval()\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_test += y_batch.size(0)\n",
    "            correct_test += (predicted == y_batch).sum().item()\n",
    "\n",
    "    test_accuracy = correct_test / total_test\n",
    "    print(f\"Test Accuracy for Split {split_idx + 1}: {test_accuracy:.4f}\")\n",
    "\n",
    "    # 保存最终的 Loss 曲线\n",
    "    plt.ioff()  # 关闭交互模式\n",
    "    plt.savefig(f\"loss_curve_split_{split_idx + 1}.png\")\n",
    "    plt.close()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "80a71797ce116114"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lsh18\\AppData\\Local\\Temp\\ipykernel_32204\\3647977593.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=device))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from ./parameter/model_split_1_epoch_10.pth\n",
      "Test Accuracy for Split 1 (Epoch 10): 0.0331\n"
     ]
    }
   ],
   "source": [
    "# 加载第10轮模型并进行测试\n",
    "split_idx = 1  # 假设我们要测试第一个数据分组（Split 1）\n",
    "epoch_to_test = 10  # 要测试的轮次\n",
    "\n",
    "# 构造模型保存路径\n",
    "model_path = f\"./parameter/model_split_{split_idx}_epoch_{epoch_to_test}.pth\"\n",
    "\n",
    "# 初始化模型\n",
    "model = LatentToTokenidModel(input_dim=input_dim, hidden_dims=hidden_dims, output_dim=output_dim).to(device)\n",
    "\n",
    "# 加载模型权重\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "print(f\"Loaded model from {model_path}\")\n",
    "\n",
    "# 从 all_data 中获取测试集\n",
    "split_data = all_data[split_idx - 1]  # split_idx 从 1 开始，数组索引从 0 开始\n",
    "test_latents = split_data[\"test_latents\"]\n",
    "test_tokenids = split_data[\"test_tokenids\"]\n",
    "\n",
    "# 转换为 PyTorch 张量\n",
    "test_dataset = TensorDataset(torch.tensor(test_latents, dtype=torch.float32),\n",
    "                              torch.tensor(test_tokenids, dtype=torch.long))\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# 测试模型\n",
    "model.eval()\n",
    "correct_test = 0\n",
    "total_test = 0\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        outputs = model(X_batch)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_test += y_batch.size(0)\n",
    "        correct_test += (predicted == y_batch).sum().item()\n",
    "\n",
    "test_accuracy = correct_test / total_test\n",
    "print(f\"Test Accuracy for Split {split_idx} (Epoch {epoch_to_test}): {test_accuracy:.4f}\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-13T21:26:15.688309400Z",
     "start_time": "2024-12-13T21:26:14.174670100Z"
    }
   },
   "id": "d35192d0260c371a"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "66e67e461412145a"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
