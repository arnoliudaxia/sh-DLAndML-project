{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arno/conda/envs/MNE-EEG/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00,  6.00it/s]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name ='/home/arno/Projects/EEGDecodingTest/My/LLM/Qwen2.5-7B-Instruct'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "prompt = \"Give me a short introduction to large language model.\"\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt} \n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "# model_inputs['inputs_embeds']=2233\n",
    "input_embeds=model.get_input_embeddings()(model_inputs['input_ids'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arno/conda/envs/MNE-EEG/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.89it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name ='/home/arno/Projects/EEGDecodingTest/My/LLM/Qwen2.5-7B-Instruct'\n",
    "device=\"cuda\"\n",
    "\n",
    "class ModifiedQwenModel(AutoModelForCausalLM):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "    \n",
    "    def forward(self, input_ids=None, attention_mask=None, input_embeds=None, **kwargs):\n",
    "        # 获取模型的 transformer\n",
    "        transformer = self.transformer\n",
    "        \n",
    "        print(input_ids.shape)\n",
    "        print(input_embeds.shape)\n",
    "        \n",
    "        if input_embeds is None:\n",
    "            hidden_states = transformer.wte(input_ids)  # 输入嵌入（Embedding）\n",
    "        else:\n",
    "            hidden_states = input_embeds\n",
    "            \n",
    "        # return hidden_states\n",
    "\n",
    "        # Apply the embedding dropout (if any)\n",
    "        if transformer.dropout is not None:\n",
    "            hidden_states = transformer.dropout(hidden_states)\n",
    "\n",
    "        # Apply position encodings\n",
    "        hidden_states = hidden_states + transformer.wpe(torch.arange(input_ids.size(1), device=input_ids.device))  # Position Embeddings\n",
    "\n",
    "        # 通过 transformer 的第2个 block 开始\n",
    "        for i in range( len(transformer.h)):  # 跳过第一个 block\n",
    "            hidden_states = transformer.h[i](hidden_states, attention_mask=attention_mask)\n",
    "\n",
    "        # 计算 logits\n",
    "        logits = transformer.lm_head(hidden_states)\n",
    "        return logits\n",
    "    \n",
    "    \n",
    "model = ModifiedQwenModel.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      " You are a helpful assistant.\n",
      "user\n",
      "日本这个国家怎么样\n",
      "assistant\n",
      "日本是一个位于东亚的岛国，拥有悠久的历史和文化。以下是一些关于日本的基本情况：\n",
      "\n",
      "1. 文化：日本文化深受中国、韩国以及西方的影响。它有许多独特的传统艺术形式，如茶道、花道、歌舞伎等。此外，动漫、流行音乐和电子游戏也是日本文化的重要组成部分。\n",
      "\n",
      "2. 经济：日本是世界第三大经济体（按购买力平价计算），以高科技产业和制造业著称。日本企业在汽车制造、电子设备等领域具有很强的竞争力。\n",
      "\n",
      "3. 科技：日本在科技创新方面处于领先地位，在机器人技术、电动汽车、可再生能源等方面取得了显著成就。\n",
      "\n",
      "4. 教育：日本重视教育，国民受教育程度较高。日本大学在全球排名中占有一定位置。\n",
      "\n",
      "5. 自然风光：日本拥有美丽的自然景观，包括富士山、温泉、樱花盛开的季节等。\n",
      "\n",
      "6. 交通：日本拥有发达的公共交通系统，包括高速铁路、地铁和公交等。\n",
      "\n",
      "7. 社会问题：尽管日本经济繁荣，但面临一些社会问题，如人口老龄化、少子化等。\n",
      "\n",
      "总之，日本是一个充满魅力的国家，具有丰富的文化和历史遗产，同时也面临着一些挑战。\n"
     ]
    }
   ],
   "source": [
    "prompt = \"日本这个国家怎么样\"\n",
    "\n",
    "messages = [\n",
    "            # {\"role\": \"system\", \"content\": ' You are a helpful assistant. The user has a language impairment, and their expressions may contain a lot of noise. You need to rephrase their meaning clearly.'},\n",
    "            {\"role\": \"system\", \"content\": ' You are a helpful assistant.'},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "# text=prompt\n",
    "\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "\n",
    "generated_ids = model.generate(model_inputs.input_ids, max_new_tokens=512, do_sample=True)\n",
    "\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system\n",
      " You are a helpful assistant.\n",
      "user\n",
      "日本这个国家怎么样\n",
      "assistant\n",
      "日本是一个位于东亚的岛国，拥有悠久的历史和文化。以下是一些关于日本的基本情况：\n",
      "\n",
      "1. 文化：日本文化深受中国、韩国以及西方的影响。它有许多独特的传统艺术形式，如茶道、花道、歌舞伎等。此外，动漫、流行音乐和电子游戏也是日本文化的重要组成部分。\n",
      "\n",
      "2. 经济：日本是世界第三大经济体（按购买力平价计算），以高科技产业和制造业著称。日本企业在汽车制造、电子设备等领域具有很强的竞争力。\n",
      "\n",
      "3. 科技：日本在科技创新方面处于领先地位，在机器人技术、电动汽车、可再生能源等方面取得了显著成就。\n",
      "\n",
      "4. 教育：日本重视教育，国民受教育程度较高。日本大学在全球排名中占有一定位置。\n",
      "\n",
      "5. 自然风光：日本拥有美丽的自然景观，包括富士山、温泉、樱花盛开的季节等。\n",
      "\n",
      "6. 交通：日本拥有发达的公共交通系统，包括高速铁路、地铁和公交等。\n",
      "\n",
      "7. 社会问题：尽管日本经济繁荣，但面临一些社会问题，如人口老龄化、少子化等。\n",
      "\n",
      "总之，日本是一个充满魅力的国家，具有丰富的文化和历史遗产，同时也面临着一些挑战。\n"
     ]
    }
   ],
   "source": [
    "prompt = \"日本这个国家怎么样\"\n",
    "\n",
    "messages = [\n",
    "            # {\"role\": \"system\", \"content\": ' You are a helpful assistant. The user has a language impairment, and their expressions may contain a lot of noise. You need to rephrase their meaning clearly.'},\n",
    "            {\"role\": \"system\", \"content\": ' You are a helpful assistant.'},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "# text=prompt\n",
    "\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "predicted_ids=model.generate(inputs_embeds=model.get_input_embeddings()(model_inputs.input_ids) ,\n",
    "                             max_new_tokens=512, do_sample=True)\n",
    "response = tokenizer.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/arno/conda/envs/MNE-EEG/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.90it/s]\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "今天的天气感觉挺暖和的。外面阳光明媚，虽然有风，但不算冷。今天出门应该没问题，挺舒服的。\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name ='/home/arno/Projects/EEGDecodingTest/My/LLM/Qwen2.5-7B-Instruct'\n",
    "device=\"cuda\"\n",
    "    \n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def getEmbedding(text, raw=False):\n",
    "        if not raw:\n",
    "                messages = [\n",
    "                        # {\"role\": \"system\", \"content\": ' You are a helpful assistant. The user has a language impairment, and their expressions may contain a lot of noise. You need to rephrase their meaning clearly.'},\n",
    "                        {\"role\": \"system\", \"content\": ' You are a helpful assistant.'},\n",
    "                        {\"role\": \"user\", \"content\": text}\n",
    "                        ]\n",
    "                text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "\n",
    "        model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "        return model.get_input_embeddings()(model_inputs.input_ids)\n",
    "\n",
    "prompt = '“天…天，天气...嗯，外面，嗯，亮，阳光，太阳，感觉…暖。嗯，风，风有点，嗯，吹，嗯，冷，是，不是...不太...不太冷。啊，今天，今天是，嗯…可以，出门，出去，走，走，没问题。嗯，暖，舒服。”'\n",
    "\n",
    "# messages = [\n",
    "#             {\"role\": \"system\", \"content\": ' You are a helpful assistant. The user has a language impairment, and their expressions may contain a lot of noise. You need to rephrase their meaning clearly.'},\n",
    "#             {\"role\": \"user\", \"content\": prompt}\n",
    "#             ]\n",
    "# text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "# model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "# generated_ids = model.generate(model_inputs.input_ids, max_new_tokens=512, do_sample=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "new_input_embed = torch.cat([getEmbedding('<|im_start|>system\\nYou are a helpful assistant. The user has a language impairment, and their expressions may contain a lot of noise. You need to rephrase their meaning clearly.<|im_end|>\\n<|im_start|>user\\n', raw=True)\n",
    "                             , getEmbedding(prompt, raw=True), \n",
    "                             getEmbedding('<|im_end|>\\n<|im_start|>assistant\\n', raw=True)], dim=1)\n",
    "\n",
    "predicted_ids=model.generate(inputs_embeds=new_input_embed ,\n",
    "                             max_new_tokens=512, do_sample=True)\n",
    "\n",
    "\n",
    "\n",
    "response = tokenizer.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3479"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "tokenids=[]\n",
    "texts=[]\n",
    "with open('/home/arno/Projects/EEGDecodingTest/My/Data/qwen-characterSplit/sub04/run_1_with_latent_tokenid.pkl', 'rb') as file:\n",
    "    loaded_data = pickle.load(file)\n",
    "    for  _, text, latent, tokenid in loaded_data:\n",
    "        tokenids.append(tokenid[0])\n",
    "        texts.append(text)\n",
    "len(tokenids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "characterEmbeddings=model.get_input_embeddings()(torch.tensor(tokenids).to(device=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0004, -0.0034,  0.0013,  ..., -0.0013,  0.0033,  0.0123],\n",
       "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "character_embeddings_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of noisy data: 6.160988414194435e-05\n",
      "Std of noisy data: 0.022888336330652237\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 假设 characterEmbeddings 是一个 PyTorch tensor，形状是 [3479, 3584]\n",
    "# 先展平数据以计算95%的置信区间\n",
    "character_embeddings_flat = characterEmbeddings.view(-1).to(torch.float32) \n",
    "\n",
    "# 计算95%置信区间\n",
    "lower_bound = torch.quantile(character_embeddings_flat, 0.025)\n",
    "upper_bound = torch.quantile(character_embeddings_flat, 0.975)\n",
    "\n",
    "# 计算新的标准差 std_new，基于95%的置信区间\n",
    "std_new = ((upper_bound - lower_bound) / (2 * 1.96)).cpu().item()\n",
    "\n",
    "# 获取均值\n",
    "mean_old = torch.mean(character_embeddings_flat).cpu().item()\n",
    "\n",
    "\n",
    "# 生成与原数据形状相同的噪声\n",
    "noise = torch.normal(mean=mean_old, std=std_new, size=characterEmbeddings.shape)\n",
    "\n",
    "# 生成带噪声的数据\n",
    "noisy_data = characterEmbeddings + noise.to(device=device)\n",
    "\n",
    "# 打印一下结果的均值和标准差，确认噪声分布\n",
    "print(f\"Mean of noisy data: {torch.mean(noisy_data).item()}\")\n",
    "print(f\"Std of noisy data: {torch.std(noisy_data).item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3479, 3584])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noisy_data.to(torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. 在六岁那年，我读过一本描绘原始森林的书，并在书中看到了一幅精彩的插图。图上画着一条蟒蛇吞吃一头猛兽的情景。书上写到蟒蛇会将猎物囫囵吞下，不咀嚼，然后躺上六个月来消化它们。当时我对丛林里的奇妙景象很感兴趣，于是用彩色铅笔画下了我的第一幅画——“作品一号”。我把这幅杰作给大人看，问他们这幅画是不是很吓人。他们回答说：“一顶帽子怎么可能会吓人呢？”我说：“我画的不是一顶帽子，而是一条蟒蛇在消化大象。”因此，我又画了蟒蛇肚子里的大象内部结构，以便让大人看得更清楚。他们总是要求我解释我的“作品二号”，但那些大人劝我不要再画蟒蛇，不管是剖开的还是没剖开的，都不要画了。他们说应该把心思放在地理、历史、算术和语法上。当时我才六岁，就放弃了辉煌的画家生涯。“作品一号”和“作品二号”都没成功，我感到泄气。那些大人自己也弄不懂我的作品，总是要孩子一遍遍地解释，这让我很烦。后来我选择了学习飞行，学会了开飞机，几乎飞遍了世界各地。地理学对我很有帮助，我可以一眼认出中国在哪里，亚利桑那在哪里。如果晚上迷路了，也很有用。我在那些大人中间生活了很久，仔细观察过他们。我发现他们似乎并没有什么特别之处。如果遇到一个头脑稍微清醒一点的人，我就拿出一直保存的“作品一号”，试着让他看看。我想知道他是否真的能看懂。但人们总回答说：“这是顶帽子。”这时，我就不再提蟒蛇、原始森林或星星之类的事情，而是说些他们能理解的事。比如桥、高尔夫、政治和领带等。\n",
      "\n",
      "2. 我孤独地生活着，没有真正谈得来的朋友。直到六年前，有一天飞机出了故障降落在撒哈拉沙漠。发动机里有东西坏了，因为身边既没有机械师也没有乘客，我打算独自完成一项困难的修复工作。这是一个生死攸关的问题。我带的水只够喝一个星期。第一天晚上，我睡在这片远离人烟的沙漠中，比在大海中漂浮的遇难者还孤独。当黎明时分，有一个奇怪的声音\n"
     ]
    }
   ],
   "source": [
    "new_input_embed = torch.cat([getEmbedding('<|im_start|>system\\nYou are a helpful assistant. The user has a language impairment, and their expressions may contain a lot of noise. You need to rephrase their meaning clearly.<|im_end|>\\n<|im_start|>user\\n', raw=True)\n",
    "                             , noisy_data.to(torch.bfloat16).unsqueeze(0), \n",
    "                             getEmbedding('<|im_end|>\\n<|im_start|>assistant\\n', raw=True)], dim=1)\n",
    "\n",
    "predicted_ids=model.generate(inputs_embeds=new_input_embed ,\n",
    "                             max_new_tokens=512, do_sample=True)\n",
    "\n",
    "\n",
    "\n",
    "response = tokenizer.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 我 六 岁 那 年 在 一 本 描 写 原 始 森 林 的 名 叫 真 实 的 故 事 的 书 上 看 见 过 一 幅 精 彩 的 插 图 画 的 是 一 条 蟒 蛇 在 吞 吃 一 头 猛 兽 我 现 在 把 它 照 样 画 在 上 面 书 中 写 道 蟒 蛇 把 猎 物 囫 囵 吞 下 嚼 都 不 嚼 然 后 它 就 无 法 动 弹 躺 上 六 个 月 来 消 化 它 们 当 时 我 对 丛 林 里 的 奇 妙 景 象 想 得 很 多 于 是 我 也 用 彩 色 铅 笔 画 了 我 的 第 一 幅 画 我 的 作 品 一 号 它 就 像 这 样 我 把 这 幅 杰 作 给 大 人 看 问 他 们 我 的 图 画 吓 不 吓 人 他 们 回 答 说 一 顶 帽 子 怎 么 会 吓 人 呢 我 画 的 不 是 一 顶 帽 子 我 画 的 是 一 条 蟒 蛇 在 消 化 大 象 于 是 我 把 蟒 蛇 肚 子 的 内 部 画 出 来 好 让 这 些 大 人 看 得 明 白 他 们 老 是 要 人 给 他 们 解 释 我 的 作 品 二 号 是 这 样 的 那 些 大 人 劝 我 别 再 画 蟒 蛇 甭 管 它 是 剖 开 的 还 是 没 剖 开 的 全 都 丢 开 他 们 说 我 还 是 把 心 思 放 在 地 理 历 史 算 术 和 语 法 上 好 就 这 样 我 才 六 岁 就 放 弃 了 辉 煌 的 画 家 生 涯 作 品 一 号 和 作 品 二 号 都 没 成 功 我 泄 了 气 那 些 大 人 自 个 儿 什 么 也 弄 不 懂 老 要 孩 子 们 一 遍 一 遍 给 他 们 解 释 真 烦 人 我 只 好 另 外 选 择 一 个 职 业 学 会 了 开 飞 机 世 界 各 地 我 差 不 多 都 飞 过 的 确 地 理 学 对 我 非 常 有 用 我 一 眼 就 能 认 出 哪 是 中 国 哪 是 亚 利 桑 那 要 是 夜 里 迷 了 路 这 很 有 用 就 这 样 我 这 一 生 中 跟 好 多 严 肃 的 人 打 过 好 多 交 道 我 在 那 些 大 人 中 间 生 活 过 很 长 时 间 我 仔 细 地 观 察 过 他 们 观 察 下 来 印 象 并 没 好 多 少 要 是 碰 上 一 个 人 看 上 去 头 脑 稍 许 清 楚 些 我 就 拿 出 一 直 保 存 着 的 作 品 一 号 让 他 试 试 看 我 想 知 道 他 是 不 是 真 的 能 看 懂 可 是 人 家 总 是 回 答 我 这 是 一 顶 帽 子 这 时 候 我 就 不 跟 他 说 什 么 蟒 蛇 啊 原 始 森 林 啊 星 星 啊 都 不 说 了 我 就 说 些 他 能 懂 的 事 情 我 跟 他 说 桥 高 尔 夫 政 治 还 有 领 带 于 是 大 人 觉 得 很 高 兴 认 识 了 这 么 个 通 情 达 理 的 人 2 我 孤 独 地 生 活 着 没 有 一 个 真 正 谈 得 来 的 人 直 到 六 年 前 有 一 次 飞 机 出 了 故 障 降 落 在 撒 哈 拉 大 沙 漠 发 动 机 里 有 样 什 么 东 西 碎 掉 了 因 为 我 身 边 既 没 有 机 械 师 也 没 有 乘 客 我 就 打 算 单 枪 匹 马 来 完 成 一 项 困 难 的 修 复 工 作 这 在 我 是 个 生 死 攸 关 的 问 题 我 带 的 水 只 够 喝 一 星 期 了 第 一 天 晚 上 我 睡 在 这 片 远 离 人 烟 的 大 沙 漠 上 比 靠 一 块 船 板 在 大 海 中 漂 流 的 遇 难 者 还 孤 独 所 以 当 天 蒙 蒙 亮 有 个 奇 怪 的 声 音 轻 轻 把 我 喊 醒 的 时 候 你 们 可 以 想 象 我 有 多 么 惊 讶 这 个 声 音 说 对 不 起 … … 请 给 我 画 只 绵 羊 ！ 嗯 ！ 请 给 我 画 只 绵 羊 … … 我 像 遭 了 雷 击 似 的 猛 地 一 下 子 跳 了 起 来 我 使 劲 地 揉 了 揉 眼 睛 仔 细 地 看 了 看 只 见 一 个 从 没 见 过 的 小 人 儿 正 一 本 正 经 地 看 着 我 呢 后 来 我 给 他 画 了 一 幅 非 常 出 色 的 肖 像 就 是 旁 边 的 这 幅 不 过 我 的 画 当 然 远 远 不 及 本 人 可 爱 这 不 是 我 的 错 我 的 画 家 生 涯 在 六 岁 那 年 就 让 大 人 给 断 送 了 除 了 画 剖 开 和 不 剖 开 的 蟒 蛇 后 来 再 没 画 过 什 么 我 吃 惊 地 瞪 大 眼 睛 瞧 着 他 你 们 别 忘 记 这 儿 离 有 人 住 的 地 方 好 远 好 远 呢 可 是 这 个 小 人 儿 看 上 去 并 不 像 迷 了 路 也 不 像 累 得 要 命 饿 得 要 命 渴 得 要 命 或 怕 得 要 命 他 一 点 不 像 在 远 离 人 类 居 住 的 沙 漠 里 迷 路 的 孩 子 等 我 总 算 说 得 出 话 时 我 对 他 说 可 是 … … 你 在 这 儿 干 吗 他 轻 声 轻 气 地 又 说 了 一 遍 好 像 那 是 件 很 要 紧 的 事 情 后 来 我 给 他 画 了 这 幅 非 常 出 色 的 肖 像 对 不 起 … … 请 给 我 画 一 只 绵 羊 … … 受 到 神 秘 事 物 强 烈 冲 击 时 一 个 人 是 不 敢 不 听 从 的 尽 管 在 我 看 来 离 一 切 有 人 居 住 的 地 方 远 而 又 远 又 处 于 死 亡 的 威 胁 之 下 在 这 儿 想 到 画 画 真 是 匪 夷 所 思 可 我 还 是 从 口 袋 里 掏 出 一 张 纸 一 支 钢 笔 但 我 想 起 我 只 学 了 地 理 历 史 算 术 和 语 法 所 以 我 就 有 点 没 好 气 地 对 那 小 人 儿 说 我 不 会 画 画 他 回 答 说 没 关 系 请 给 我 画 一 只 绵 羊 我 因 为 从 没 画 过 绵 羊 就 在 我 只 会 画 的 两 张 图 画 里 挑 一 张 给 他 画 了 没 剖 开 的 蟒 蛇 图 可 我 听 到 小 人 儿 下 面 说 的 话 简 直 惊 呆 了 不 对 ！ 不 对 ！ 我 不 要 在 蟒 蛇 肚 子 里 的 大 象 蟒 蛇 很 危 险 大 象 呢 太 占 地 方 在 我 那 儿 什 么 都 是 小 小 的 我 要 的 是 一 只 绵 羊 请 给 我 画 一 只 绵 羊 我 只 得 画 了 起 来 他 专 心 地 看 了 一 会 儿 然 后 说 不 对 ！ 这 只 羊 已 经 病 得 不 轻 了 另 外 画 一 只 吧 我 画 了 右 面 的 这 只 我 的 朋 友 温 和 地 笑 了 口 气 宽 容 地 说 你 看 看 … … 这 只 不 是 绵 羊 是 山 羊 头 上 长 着 角 … … 于 是 我 又 画 了 一 张 但 这 一 张 也 跟 前 几 张 一 样 没 能 通 过 这 只 太 老 了 我 要 一 只 可 以 活 得 很 久 的 绵 羊 我 已 经 没 有 耐 心 了 因 为 我 急 于 要 去 把 发 动 机 拆 下 来 所 以 我 就 胡 乱 画 了 一 张 我 随 口 说 道 这 个 呢 是 个 箱 子 你 要 的 绵 羊 就 在 里 面 但 是 令 我 吃 惊 的 是 这 个 小 评 判 的 脸 上 顿 时 变 得 容 光 焕 发 了 我 要 的 就 是 这 个 ！ 你 说 这 只 绵 羊 会 要 很 多 草 吗 问 这 干 嘛 因 为 我 那 儿 样 样 都 很 小 … … 肯 定 够 了 我 给 你 的 是 只 很 小 的 绵 羊 他 低 下 头 去 看 那 幅 画 不 算 太 小 … … 瞧 ！ 它 睡 着 了 … … 就 这 样 我 认 识 了 小 王 子 3 很 久 以 后 我 才 弄 明 白 他 是 从 哪 儿 来 的 这 个 小 王 子 对 我 提 了 好 多 问 题 而 对 我 的 问 题 总 像 没 听 见 似 的 我 是 从 他 偶 尔 漏 出 来 的 那 些 话 里 一 点 一 点 知 道 这 一 切 的 比 如 他 第 一 次 瞧 见 我 的 飞 机 时 我 没 画 我 的 飞 机 对 我 来 说 这 样 的 画 实 在 太 复 杂 了 就 问 我 这 是 什 么 东 西 这 不 是 什 么 东 西 它 会 飞 这 是 一 架 飞 机 是 我 的 飞 机 我 自 豪 地 讲 给 他 听 我 在 天 上 飞 他 听 了 就 大 声 说 怎 么 ！ 你 是 天 上 掉 下 来 的 是 的 我 谦 虚 地 说 喔 ！ 真 有 趣 … … 小 王 子 发 出 一 阵 清 脆 的 笑 声 这 下 可 把 我 惹 恼 了 我 不 喜 欢 别 人 拿 我 的 不 幸 逗 趣 儿 接 着 他 又 说 这 么 说 你 也 是 从 天 上 来 的 ！ 你 从 哪 个 星 球 来 我 脑 子 里 闪 过 一 个 念 头 他 的 降 临 之 谜 好 像 有 了 线 索 我 突 如 其 来 地 发 问 那 你 是 从 别 的 星 球 来 的 啰 可 是 他 没 有 回 答 他 看 着 我 的 飞 机 轻 轻 地 点 了 点 头 是 啊 就 靠 它 你 来 的 地 方 不 会 太 远 … … 说 着 他 出 神 地 遐 想 了 很 久 而 后 从 袋 里 拿 出 我 画 的 绵 羊 全 神 贯 注 地 凝 望 着 这 宝 贝 你 想 想 看 这 个 跟 别 的 星 球 有 关 说 了 一 半 打 住 的 话 头 会 让 我 多 么 惊 讶 啊 我 竭 力 想 多 知 道 一 些 你 从 哪 儿 来 我 的 小 家 伙 ‘ 我 那 儿 ’ 是 哪 儿 你 要 把 我 画 的 绵 羊 带 到 哪 儿 去 他 若 有 所 思 地 沉 默 了 一 会 儿 然 后 开 口 对 我 说 你 给 了 我 这 个 箱 子 这 就 好 了 晚 上 可 以 给 它 当 屋 子 当 然 要 是 你 乖 我 还 会 给 你 一 根 绳 子 白 天 可 以 把 它 拴 住 木 桩 也 有 这 个 提 议 好 像 使 小 王 子 很 不 以 为 然 拴 住 真 是 怪 念 头 ！ 可 要 是 你 不 把 它 拴 住 它 就 会 到 处 跑 还 会 跑 丢 了 … … 小 王 子 在 B 6 1 2 小 行 星 上 我 的 朋 友 又 咯 咯 地 笑 了 起 来 你 叫 它 往 哪 儿 跑 呀 到 处 跑 笔 直 往 前 … … 这 时 小 王 子 一 本 正 经 地 说 那 也 没 关 系 我 那 儿 就 一 丁 点 儿 大 ！ 然 后 他 又 说 了 一 句 语 气 中 仿 佛 有 点 儿 忧 郁 就 是 笔 直 往 前 跑 也 跑 不 了 多 远 … … 4 我 由 此 知 道 了 另 一 件 很 重 要 的 事 情 他 居 住 的 星 球 比 一 座 房 子 大 不 了 多 少 ！ 这 并 没 让 我 感 到 很 吃 惊 我 知 道 除 了 像 地 球 木 星 火 星 金 星 这 些 取 了 名 字 的 大 星 球 还 有 成 千 上 万 的 星 球 它 们 有 时 候 非 常 非 常 小 用 望 远 镜 都 不 大 看 得 见 天 文 学 家 找 到 其 中 的 一 个 星 球 给 它 编 一 个 号 码 就 算 名 字 了 比 如 说 他 把 它 叫 作 3 2 5 1 号 小 行 星 我 有 很 可 靠 的 理 由 足 以 相 信 小 王 子 原 先 住 的 那 个 星 球 就 是 B 6 1 2 号 小 行 星 这 颗 小 行 星 只 在 一 九 〇 九 年 被 人 用 望 远 镜 望 见 过 一 次 那 人 是 一 个 土 耳 其 天 文 学 家 当 时 他 在 一 次 国 际 天 文 学 大 会 上 作 了 长 篇 论 证 可 是 就 为 了 他 的 服 装 的 缘 故 谁 也 不 信 他 的 话 大 人 哪 就 是 这 样 幸 好 有 一 个 土 耳 其 独 裁 者 下 令 全 国 百 姓 都 要 穿 欧 洲 的 服 装 违 令 者 处 死 这 一 下 B 6 1 2 号 小 行 星 的 名 声 总 算 保 全 了 那 个 天 文 学 家 在 一 九 二 〇 年 重 新 作 报 告 穿 着 一 套 非 常 体 面 的 西 装 这 一 回 所 有 的 人 都 同 意 了 他 的 观 点 我 之 所 以 要 跟 你 们 一 五 一 十 地 介 绍 B 6 1 2 号 小 行 星 还 把 它 的 编 号 也 讲 得 明 明 白 白 完 全 是 为 了 大 人 那 些 大 人 就 喜 欢 数 字 你 跟 他 们 讲 起 一 个 新 朋 友 他 们 总 爱 问 些 无 关 紧 要 的 问 题 他 们 不 会 问 你 他 说 话 的 声 音 是 怎 样 的 他 喜 欢 玩 哪 些 游 戏 他 是 不 是 收 集 蝴 蝶 标 本 他 们 问 的 是 他 几 岁 有 几 个 兄 弟 他 有 多 重 他 父 亲 挣 多 少 钱 这 样 问 过 以 后 他 们 就 以 为 了 解 他 了 你 要 是 对 大 人 说 我 看 见 一 幢 漂 亮 的 房 子 红 砖 墙 窗 前 种 着 天 竺 葵 屋 顶 上 停 着 鸽 子 … … 他 们 想 象 不 出 这 幢 房 子 是 怎 样 的 你 得 这 么 跟 他 们 说 我 看 见 一 幢 十 万 法 郎 的 房 子 他 们 马 上 会 大 声 嚷 嚷 多 漂 亮 的 房 子 ！ 所 以 如 果 你 对 他 们 说 小 王 子 是 存 在 的 证 据 就 是 他 那 么 可 爱 他 咯 咯 地 笑 他 还 想 要 一 只 绵 羊 一 个 人 想 要 有 只 绵 羊 这 就 是 他 存 在 的 证 据 嘛 他 们 会 耸 耸 肩 膀 只 当 你 还 是 个 孩 子 ！ 可 要 是 你 对 他 们 说 他 来 自 B 6 1 2 号 小 行 星 他 们 就 会 深 信 不 疑 不 再 问 这 问 那 地 烦 你 了 他 们 就 是 这 样 不 必 怪 他 们 孩 子 应 该 对 大 人 多 多 原 谅 才 是 不 过 当 然 我 们 懂 得 生 活 我 们 才 不 把 数 字 放 在 眼 里 呢 ！ 我 真 愿 意 像 讲 童 话 那 样 来 开 始 讲 这 个 故 事 我 真 想 这 样 说 从 前 呀 有 一 个 小 王 子 住 在 一 个 跟 他 身 体 差 不 多 大 的 星 球 上 他 想 有 个 朋 友 … … 对 那 些 懂 得 生 活 的 人 来 说 这 样 听 上 去 会 真 实 得 多 我 不 想 人 家 轻 率 地 来 读 我 这 本 书 我 讲 述 这 段 往 事 时 心 情 是 很 难 过 的 我 的 朋 友 带 着 他 的 绵 羊 已 经 离 去 六 年 了 我 之 所 以 在 这 儿 细 细 地 描 述 他 就 是 为 了 不 要 忘 记 他 忘 记 朋 友 是 件 令 人 伤 心 的 事 情 并 不 是 人 人 都 有 过 一 个 朋 友 的 再 说 我 早 晚 也 会 变 得 像 那 些 只 关 心 数 字 的 大 人 一 样 的 也 正 是 为 了 这 个 缘 故 我 买 了 一 盒 颜 料 和 一 些 铅 笔 到 了 我 这 年 纪 再 重 握 画 笔 是 挺 费 劲 的 况 且 当 初 我 只 画 过 剖 开 和 没 剖 开 的 蟒 蛇 还 是 六 岁 那 年 ！ 当 然 我 一 定 要 尽 力 把 它 们 画 得 像 一 些 但 做 不 做 得 到 我 可 说 不 准 有 时 这 一 张 还 行 那 一 张 就 不 大 像 了 比 如 说 身 材 我 就 有 点 记 不 准 确 了 这 一 张 里 小 王 子 画 得 太 高 了 那 一 张 呢 太 矮 了 衣 服 的 颜 色 也 挺 让 我 犯 难 我 只 好 信 手 拿 起 色 笔 这 儿 试 一 下 那 儿 试 一 下 到 头 来 有 些 最 要 紧 的 细 部 说 不 定 都 弄 错 了 不 过 这 一 切 大 家 都 得 原 谅 我 才 是 我 的 朋 友 从 来 不 跟 我 解 释 什 么 他 大 概 以 为 我 是 跟 他 一 样 的 可 是 很 遗 憾 我 已 经 瞧 不 见 箱 子 里 面 的 绵 羊 了 我 也 许 已 经 有 点 像 那 些 大 人 了 我 一 定 是 老 了\n"
     ]
    }
   ],
   "source": [
    "print(*texts)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MNE-EEG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
